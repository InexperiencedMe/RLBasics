{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "torch.set_printoptions(linewidth=120, precision=2, sci_mode=False)\n",
    "np.set_printoptions(linewidth=120, precision=2, suppress=True)\n",
    "import cv2 as cv\n",
    "from mlagents_envs.environment import UnityEnvironment, BaseEnv, ActionTuple\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from gymnasium import spaces\n",
    "from numpy import inf\n",
    "\n",
    "seed = 1\n",
    "torch_deterministic = True\n",
    "totalTimesteps = 1000000\n",
    "buffer_size = int(1e6)\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 512\n",
    "policy_lr = 3e-4\n",
    "q_lr = 1e-3\n",
    "policyFrequency = 4\n",
    "QNetworkFrequency = 2  # Denis Yarats' implementation delays this by 2.\n",
    "alpha = 0.2\n",
    "autoEntropy = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, obsSize, actionSize):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obsSize + actionSize, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # print(f\"x: {x} of shape {x.shape}, a: {a} of shape {a.shape} and we will try to concatenate them\")\n",
    "        x = x.to(torch.float)\n",
    "        print(f\"Shape of x {x.shape}, shape of a {a.shape}\")\n",
    "        if len(x.shape) > len(a.shape):\n",
    "            a = a.unsqueeze(len(x.shape)-2)\n",
    "        x = torch.cat([x, a], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obsSize, actionSize, actionLowBound, actionHighBound):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obsSize, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.actionSpace = actionSize\n",
    "        self.fc_mean = nn.Linear(256, actionSize)\n",
    "        self.fc_logstd = nn.Linear(256, actionSize)\n",
    "        self.register_buffer(\"action_scale\", torch.tensor((actionHighBound - actionLowBound) / 2.0, dtype=torch.float32))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor((actionHighBound + actionLowBound) / 2.0, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = torch.clamp(self.fc_logstd(x), LOG_STD_MIN, LOG_STD_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x, evaluation=False):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        # print(f\"mean, std shapes: {mean.shape}, {std.shape}\")\n",
    "        policyDistribution = torch.distributions.Normal(mean, std)\n",
    "        if evaluation:\n",
    "            actionSample = mean\n",
    "        else:\n",
    "            actionSample = policyDistribution.rsample()\n",
    "        # print(f\"actionSample.shape {actionSample.shape}\")\n",
    "        tanhAction = torch.tanh(actionSample)\n",
    "        # print(f\"tanhAction.shape {tanhAction.shape}\")\n",
    "        action = tanhAction * self.action_scale + self.action_bias\n",
    "        # print(f\"action.shape {action.shape}\")\n",
    "        log_prob = policyDistribution.log_prob(actionSample)\n",
    "        # print(f\"log_prob.shape {log_prob.shape}, log_prob {log_prob}\")\n",
    "        # print(f\"tanhAction.shape {tanhAction.shape}, tanhAction {tanhAction}\")\n",
    "        # print(f\"action_scale.shape {self.action_scale.shape}, action_scale {self.action_scale}\")\n",
    "        # print(f\"Trying to add to log prob log of self.action_scale * (1 - tanhAction.pow(2)) + 1e-6: {self.action_scale * (1 - tanhAction.pow(2)) + 1e-6}\")\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - tanhAction.pow(2)) + 1e-6)\n",
    "        # print(f\"Added it, so now log_prob is {log_prob} of shape {log_prob.shape}\")\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        # print(f\"Summed it so now logprob is {log_prob} of shape {log_prob.shape}\")\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.behavior_specs: <mlagents_envs.base_env.BehaviorMapping object at 0x0000023E1D803100> So first element will be our behaviorName\n",
      "All specs: BehaviorSpec(observation_specs=[ObservationSpec(shape=(126,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='PhysicsBodySensor:Body'), ObservationSpec(shape=(32,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size32')], action_spec=ActionSpec(continuous_size=20, discrete_branches=()))\n",
      "We have 20 continuous action size\n",
      "We have 0 branches of () continuous action sizes respectively\n"
     ]
    }
   ],
   "source": [
    "environmentChannel = EnvironmentParametersChannel()\n",
    "# engineChannel = EnvironmentParametersChannel() for now I can only have one channel at a time\n",
    "env = UnityEnvironment(file_name=None, side_channels=[environmentChannel])\n",
    "# channel.set_float_parameter(\"parameter_1\", 2.0)\n",
    "env.reset()\n",
    "\n",
    "print(f\"env.behavior_specs: {env.behavior_specs} So first element will be our behaviorName\")\n",
    "behaviorName = list(env.behavior_specs)[0]\n",
    "specs = env.behavior_specs[behaviorName]\n",
    "print(f\"All specs: {specs}\")\n",
    "\n",
    "env.reset()\n",
    "decisionSteps, terminalSteps = env.get_steps(behaviorName)\n",
    "agentID = decisionSteps.agent_id[0] # I dont get that part yet\n",
    "obs = decisionSteps.obs[agentID] # obs have shape 1, 1, 10 for now: nAgents, nStackedVectors, obsSize\n",
    "\n",
    "continuousActionSize = specs.action_spec.continuous_size\n",
    "print(f\"We have {continuousActionSize} continuous action size\")\n",
    "print(f\"We have {specs.action_spec.discrete_size} branches of {specs.action_spec.discrete_branches} discrete action sizes respectively\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10,) into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m decisionSteps\u001b[38;5;241m.\u001b[39mobs[agentID]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print(f\"### Pushing {obs, next_obs, actionsContinuous} to experience as first 3 elements\")\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mexperiences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactionsContinuous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodicReward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magentID\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mterminalSteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[0;32m     66\u001b[0m sampledExperiences \u001b[38;5;241m=\u001b[39m experiences\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(batch_size, experiences\u001b[38;5;241m.\u001b[39msize()))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:263\u001b[0m, in \u001b[0;36mReplayBuffer.add\u001b[1;34m(self, obs, next_obs, action, reward, done, infos)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_observations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_obs)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(done)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_timeout_termination:\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (10,) into shape (1,)"
     ]
    }
   ],
   "source": [
    "observationSize = np.array(specs.observation_specs[agentID].shape).prod()\n",
    "continuousActionSize = specs.action_spec.continuous_size\n",
    "\n",
    "actor = Actor(observationSize, continuousActionSize, 0, 1).to(device)\n",
    "QNet1 = SoftQNetwork(observationSize, continuousActionSize).to(device)\n",
    "QNet2 = SoftQNetwork(observationSize, continuousActionSize).to(device)\n",
    "QNet1_target = SoftQNetwork(observationSize, continuousActionSize).to(device)\n",
    "QNet2_target = SoftQNetwork(observationSize, continuousActionSize).to(device)\n",
    "QNet1_target.load_state_dict(QNet1.state_dict())\n",
    "QNet2_target.load_state_dict(QNet2.state_dict())\n",
    "QNetsOptimizer = optim.Adam(list(QNet1.parameters()) + list(QNet2.parameters()), lr=q_lr)\n",
    "actorOptimizer = optim.Adam(list(actor.parameters()), lr=policy_lr)\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if autoEntropy:\n",
    "    target_entropy = -torch.tensor(continuousActionSize).item()\n",
    "    logAlpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = logAlpha.exp().item()\n",
    "    alphaOptimizer = optim.Adam([logAlpha], lr=q_lr)\n",
    "\n",
    "nEnvs = 10\n",
    "experiences = ReplayBuffer(\n",
    "    buffer_size,\n",
    "    spaces.Box(low=-inf, high=inf, shape=(nEnvs, observationSize), dtype=np.float32),\n",
    "    spaces.Box(low=-inf, high=inf, shape=(nEnvs, continuousActionSize), dtype=np.float32),\n",
    "    device,\n",
    "    handle_timeout_termination=False\n",
    "    # n_envs=10\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "decisionSteps, terminalSteps = env.get_steps(behaviorName)\n",
    "agentID = decisionSteps.agent_id[0] # I dont get that part yet\n",
    "obs = decisionSteps.obs[agentID] # obs have shape 1, 1, 10 for now: nAgents, nStackedVectors, obsSize\n",
    "\n",
    "allScores = []\n",
    "episodicReward = 0.0\n",
    "for globalStep in range(totalTimesteps):\n",
    "    actionsContinuous, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "    actionsContinuous = actionsContinuous.detach().cpu().numpy()\n",
    "    actions = ActionTuple(continuous=actionsContinuous)\n",
    "\n",
    "    # print(f\"Based on observation {obs} we take action {actionsContinuous}\")\n",
    "\n",
    "    env.set_actions(behaviorName, actions)\n",
    "    env.step()\n",
    "    decisionSteps, terminalSteps = env.get_steps(behaviorName)\n",
    "\n",
    "\n",
    "\n",
    "    if agentID in terminalSteps:\n",
    "        next_obs = terminalSteps.obs[agentID]\n",
    "        print(f\"Total episodic reward: {episodicReward}\")\n",
    "        allScores.append(episodicReward)\n",
    "        episodicReward = 0\n",
    "        env.reset()\n",
    "    else:\n",
    "        episodicReward += decisionSteps.reward\n",
    "        next_obs = decisionSteps.obs[agentID]\n",
    "\n",
    "    # print(f\"### Pushing {obs, next_obs, actionsContinuous} to experience as first 3 elements\")\n",
    "    experiences.add(obs, next_obs, actionsContinuous, episodicReward, True if agentID in terminalSteps else False, \"\")\n",
    "    obs = next_obs\n",
    "\n",
    "\n",
    "    sampledExperiences = experiences.sample(min(batch_size, experiences.size()))\n",
    "    with torch.no_grad():\n",
    "        nextStateActions, nextStateLogProbs, _ = actor.get_action(sampledExperiences.next_observations)\n",
    "        # print(f\"### Actor just spit out {nextStateActions} from input {sampledExperiences.next_observations}\")\n",
    "        # print(f\"### So we're giving QNet: {sampledExperiences.next_observations} and {nextStateActions} and its fine\")\n",
    "        QFunction1NextTarget = QNet1_target(sampledExperiences.next_observations, nextStateActions)\n",
    "        QFunction2NextTarget = QNet2_target(sampledExperiences.next_observations, nextStateActions)\n",
    "        minQNextTarget = torch.min(QFunction1NextTarget, QFunction2NextTarget) - alpha * nextStateLogProbs\n",
    "        nextQValue = sampledExperiences.rewards.flatten() + (1 - sampledExperiences.dones.flatten()) * gamma * (minQNextTarget).view(-1)\n",
    "\n",
    "\n",
    "    # print(f\"### But we're giving QNet: {sampledExperiences.observations} and {sampledExperiences.actions} and its NOTTTT fine\")\n",
    "    QFunction1ActionValues = QNet1(sampledExperiences.observations, sampledExperiences.actions).view(-1)\n",
    "    QFunction2ActionValues = QNet2(sampledExperiences.observations, sampledExperiences.actions).view(-1)\n",
    "    QFunction1Loss = F.mse_loss(QFunction1ActionValues, nextQValue)\n",
    "    QFunction2Loss = F.mse_loss(QFunction2ActionValues, nextQValue)\n",
    "    QFunctionsTotalLoss = QFunction1Loss + QFunction2Loss\n",
    "\n",
    "    QNetsOptimizer.zero_grad()\n",
    "    QFunctionsTotalLoss.backward()\n",
    "    QNetsOptimizer.step()\n",
    "\n",
    "    if globalStep % policyFrequency == 0:\n",
    "        for i in range(policyFrequency):\n",
    "            if i > 0:\n",
    "                # Sample new experiences if we make multiple updates\n",
    "                sampledExperiences = experiences.sample(min(batch_size, experiences.size()))\n",
    "            actions, logProbabilities, _ = actor.get_action(sampledExperiences.observations)\n",
    "            QFunction1Evaluation = QNet1(sampledExperiences.observations, actions)\n",
    "            QFunction2Evaluation = QNet2(sampledExperiences.observations, actions)\n",
    "            minQEvalutaion = torch.min(QFunction1Evaluation, QFunction2Evaluation)\n",
    "            actorLoss = ((alpha * logProbabilities) - minQEvalutaion).mean()\n",
    "\n",
    "            actorOptimizer.zero_grad()\n",
    "            actorLoss.backward()\n",
    "            actorOptimizer.step()\n",
    "\n",
    "            if autoEntropy:\n",
    "                with torch.no_grad():\n",
    "                    _, logProbabilities, _ = actor.get_action(sampledExperiences.observations)\n",
    "                alphaLoss = (-logAlpha.exp()*(logProbabilities + target_entropy)).mean()\n",
    "\n",
    "                alphaOptimizer.zero_grad()\n",
    "                alphaLoss.backward()\n",
    "                alphaOptimizer.step()\n",
    "                alpha = logAlpha.exp().item()\n",
    "\n",
    "    if globalStep % QNetworkFrequency == 0:\n",
    "        for param, targetParam in zip(QNet1.parameters(), QNet1_target.parameters()):\n",
    "            targetParam.data.copy_(tau*param.data + (1 - tau)*targetParam.data)\n",
    "        for param, targetParam in zip(QNet2.parameters(), QNet2_target.parameters()):\n",
    "            targetParam.data.copy_(tau*param.data + (1 - tau)*targetParam.data)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
