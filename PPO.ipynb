{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "gymID = \"LunarLander-v2\"\n",
    "nEnvs = 8\n",
    "totalNSteps = 4000000\n",
    "nStepsPerRollout = 64\n",
    "batchSize = int(nStepsPerRollout*nEnvs)\n",
    "nMinibatches = 2\n",
    "minibatchSize = int(batchSize // nMinibatches)\n",
    "LearningRate = 3e-3\n",
    "annealLR = True\n",
    "gamma = 0.999\n",
    "nUpdateEpochs = 4\n",
    "clipCoefficient = 0.2\n",
    "entropyLossCoefficient = 0.02\n",
    "valueLossCoefficient = 0.5\n",
    "maxGradNorm = 0.5\n",
    "normAdvantages = True\n",
    "gaeLambda = 0.99 # general advantage estimation\n",
    "\n",
    "torch.set_printoptions(linewidth=120, precision=2, sci_mode=False, profile=\"short\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveVideo(images, outputDirectory, fps):\n",
    "    height, width, _ = images[0].shape\n",
    "    writer = cv.VideoWriter(outputDirectory, cv.VideoWriter_fourcc(*'H264'), fps, (width, height))\n",
    "    for image in images:\n",
    "        bgr_image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "        writer.write(bgr_image)\n",
    "    writer.release()\n",
    "\n",
    "@torch.no_grad()\n",
    "def recordVideoOfAgent(agent, environmentName, filename, fps=30):\n",
    "    images = []\n",
    "    environment = gym.make(environmentName, render_mode=\"rgb_array\")\n",
    "    for i in range(1):\n",
    "        state, _ = environment.reset()\n",
    "        totalReward = 0\n",
    "        while True:\n",
    "            action, _, _, _ = agent.get_action_and_value(torch.tensor(state).to(device))\n",
    "            nextState, reward, terminated, truncated, _ = environment.step(action.cpu().numpy())\n",
    "            nextState = torch.tensor(nextState, dtype=torch.float32)\n",
    "            totalReward += reward\n",
    "            images.append(environment.render())\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(f\"Final reward: {totalReward:>8.2f}\")\n",
    "                break\n",
    "\n",
    "            state = nextState\n",
    "    saveVideo(images, f\"{filename}-({totalReward:.0f}).mp4\", fps)\n",
    "\n",
    "def splitNumber(number, nSplits):\n",
    "    step = number / (nSplits - 1)\n",
    "    indices = [0]\n",
    "    for i in range(1, nSplits - 1):\n",
    "        indices.append(round(step * i))\n",
    "    indices.append(number - 1)\n",
    "    return indices\n",
    "\n",
    "def linearInitialize(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            linearInitialize(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 256)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(256, 128)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(128, 64)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(64, 1), std=1.0))\n",
    "        self.actor = nn.Sequential(\n",
    "            linearInitialize(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 256)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(256, 128)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(128, 64)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(64, envs.single_action_space.n), std=0.01))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probabilities = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probabilities.sample()\n",
    "        return action, probabilities.log_prob(action), probabilities.entropy(), self.critic(x)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "envs = gym.vector.make(gymID, num_envs=nEnvs)\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LearningRate, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((nStepsPerRollout, nEnvs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((nStepsPerRollout, nEnvs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((nStepsPerRollout, nEnvs)).to(device)\n",
    "rewards = torch.zeros((nStepsPerRollout, nEnvs)).to(device)\n",
    "dones = torch.zeros((nStepsPerRollout, nEnvs)).to(device)\n",
    "values = torch.zeros((nStepsPerRollout, nEnvs)).to(device)\n",
    "\n",
    "globalStep = 0\n",
    "startTime = time.time()\n",
    "nextObs, _ = envs.reset(seed=seed)\n",
    "nextObs = torch.tensor(nextObs).to(device)\n",
    "nextDone = torch.zeros(nEnvs).to(device)\n",
    "nUpdates = int(totalNSteps // batchSize)\n",
    "\n",
    "totalEpisodicRewards = torch.zeros(nEnvs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores = []\n",
    "for update in range(1, nUpdates + 1):\n",
    "    if annealLR:\n",
    "        frac = 1.0 - (update - 1.0) / nUpdates\n",
    "        lrnow = frac * LearningRate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, nStepsPerRollout):\n",
    "        globalStep += 1 * nEnvs\n",
    "        obs[step] = nextObs\n",
    "        dones[step] = nextDone\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(nextObs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        \n",
    "        nextObs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or.reduce([terminated, truncated])\n",
    "        rewards[step] = torch.tensor(reward).to(device)\n",
    "        nextObs, nextDone = torch.Tensor(nextObs).to(device), torch.Tensor(done).to(device)\n",
    "            \n",
    "        totalEpisodicRewards += torch.tensor(reward).to(device)\n",
    "        for finalScore in totalEpisodicRewards[done]:\n",
    "            print(f\"Score: {finalScore:>8.2f}\")\n",
    "            allScores.append(finalScore.item())\n",
    "        totalEpisodicRewards[done] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nextValue = agent.get_value(nextObs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(nStepsPerRollout)):\n",
    "            if t == nStepsPerRollout - 1:\n",
    "                nextnonterminal = 1.0 - nextDone\n",
    "                nextvalues = nextValue\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + gamma * gaeLambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    batchObs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    batchLogProbabilities = logprobs.reshape(-1)\n",
    "    batchActions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    batchAdvantages = advantages.reshape(-1)\n",
    "    batchReturns = returns.reshape(-1)\n",
    "    batchValues = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    batchIndices = np.arange(batchSize)\n",
    "    for epoch in range(nUpdateEpochs):\n",
    "        np.random.shuffle(batchIndices)\n",
    "        for start in range(0, batchSize, minibatchSize):\n",
    "            end = start + minibatchSize\n",
    "            miniBatchIndices = batchIndices[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(batchObs[miniBatchIndices], batchActions.long()[miniBatchIndices])\n",
    "            logratio = newlogprob - batchLogProbabilities[miniBatchIndices]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            minibatchAdvantages = batchAdvantages[miniBatchIndices]\n",
    "            if normAdvantages:\n",
    "                minibatchAdvantages = (minibatchAdvantages - minibatchAdvantages.mean()) / (minibatchAdvantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            policyLoss = -minibatchAdvantages * ratio\n",
    "            policyLossClipped = -minibatchAdvantages * torch.clamp(ratio, 1 - clipCoefficient, 1 + clipCoefficient)\n",
    "            policyLossFinal = torch.max(policyLoss, policyLossClipped).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            valueLossUnclipped = (newvalue - batchReturns[miniBatchIndices]) ** 2\n",
    "            valueLossClipped = batchValues[miniBatchIndices] + torch.clamp(newvalue - batchValues[miniBatchIndices], -clipCoefficient, clipCoefficient)\n",
    "            valueLossClipped = (valueLossClipped - batchReturns[miniBatchIndices]) ** 2\n",
    "            valueLoss = torch.max(valueLossUnclipped, valueLossClipped)\n",
    "            valueLoss = 0.5 * valueLoss.mean() # Supposedly 0.5 is to cancel out ** 2 and speed up backprop\n",
    "\n",
    "            entropyLoss = entropy.mean()\n",
    "            loss = policyLossFinal + valueLossCoefficient*valueLoss - entropyLossCoefficient*entropyLoss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), maxGradNorm)\n",
    "            optimizer.step()\n",
    "    \n",
    "    # if update in splitNumber(nUpdates, 20) or update == nUpdates or update == 1:\n",
    "    #     recordVideoOfAgent(agent, gymID, f\"videos/{gymID}-{globalStep/1000:.0f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def movingAveragePlot(values, averagingValue):\n",
    "    plt.plot(range(len(values)), values, label=\"scores\")\n",
    "    plt.plot([i*averagingValue for i in range(len(values)//averagingValue)], torch.tensor(values[len(values)%averagingValue:]).view(-1, averagingValue).mean(1), label=f\"averaged {averagingValue}\")\n",
    "\n",
    "movingAveragePlot(allScores, 20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
