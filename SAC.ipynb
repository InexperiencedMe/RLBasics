{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "torch.set_printoptions(linewidth=120, precision=2, sci_mode=False, profile=\"short\")\n",
    "import cv2 as cv\n",
    "\n",
    "# Doesnt support more than 1 env yet\n",
    "# I dont think sampledExperienced.dones works at all, where do the names come from? Remake it with namedTuple\n",
    "\n",
    "seed = 1\n",
    "nEnvs = 1\n",
    "torch_deterministic = True\n",
    "env_id = \"LunarLander-v2\"\n",
    "totalTimesteps = 10\n",
    "buffer_size = int(1e6)\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 256\n",
    "policy_lr = 3e-4\n",
    "q_lr = 1e-3\n",
    "policyFrequency = 2\n",
    "QNetworkFrequency = 1  # Denis Yarats' implementation delays this by 2.\n",
    "noise_clip = 0.5\n",
    "alpha = 0.2\n",
    "autoEntropy = True\n",
    "targetEntropyScale = 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "def linearInitialize(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            linearInitialize(nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(256, 128)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(128, 64)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(64, 1), std=1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            linearInitialize(nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(256, 128)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(128, 64)),\n",
    "            nn.Tanh(),\n",
    "            linearInitialize(nn.Linear(64, env.single_action_space.n), std=0.01))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actor(x)\n",
    "\n",
    "    def get_action(self, x):\n",
    "        logits = self(x)\n",
    "        policyDistribution = Categorical(logits=logits)\n",
    "        action = policyDistribution.sample()\n",
    "        actionProbabilities = policyDistribution.probs\n",
    "        logActionProbabilities = F.log_softmax(logits, dim=1)\n",
    "        return action, logActionProbabilities, actionProbabilities\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "envs = gym.vector.make(env_id, num_envs=nEnvs)\n",
    "envs.single_observation_space.dtype = np.float32\n",
    "totalEpisodicRewards = torch.zeros(nEnvs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m globalStep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(totalTimesteps):\n\u001b[1;32m---> 29\u001b[0m     actions, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     32\u001b[0m     next_obs, rewards, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m, in \u001b[0;36mActor.get_action\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m policyDistribution \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m     43\u001b[0m action \u001b[38;5;241m=\u001b[39m policyDistribution\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 44\u001b[0m actionProbabilities \u001b[38;5;241m=\u001b[39m \u001b[43mpolicyDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m logActionProbabilities \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, logActionProbabilities, actionProbabilities\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "actor = Actor(envs).to(device)\n",
    "QNet1 = SoftQNetwork(envs).to(device)\n",
    "QNet2 = SoftQNetwork(envs).to(device)\n",
    "QNet1_target = SoftQNetwork(envs).to(device)\n",
    "QNet2_target = SoftQNetwork(envs).to(device)\n",
    "QNet1_target.load_state_dict(QNet1.state_dict())\n",
    "QNet2_target.load_state_dict(QNet2.state_dict())\n",
    "QNetsOptimizer = optim.Adam(list(QNet1.parameters()) + list(QNet2.parameters()), lr=q_lr)\n",
    "actorOptimizer = optim.Adam(list(actor.parameters()), lr=policy_lr)\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if autoEntropy:\n",
    "    targetEntropy = -targetEntropyScale * torch.log(1 / torch.tensor(envs.single_action_space.n))\n",
    "    logAlpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = logAlpha.exp().item()\n",
    "    alphaOptimizer = optim.Adam([logAlpha], lr=q_lr)\n",
    "\n",
    "experiences = ReplayBuffer(\n",
    "    buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "\n",
    "allScores = []\n",
    "obs, _ = envs.reset(seed=seed)\n",
    "for globalStep in range(totalTimesteps):\n",
    "    actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "    actions = actions.detach().cpu().numpy()\n",
    "\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    experiences.add(obs, next_obs, actions, rewards, terminations, infos)\n",
    "    obs = next_obs\n",
    "\n",
    "    done = np.logical_or.reduce([terminations, truncations])\n",
    "    totalEpisodicRewards += torch.tensor(rewards).to(device)\n",
    "    for finalScore in totalEpisodicRewards[done]:\n",
    "        print(f\"Score: {finalScore:>8.2f}\")\n",
    "        allScores.append(finalScore.item())\n",
    "    totalEpisodicRewards[done] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sampledExperiences = experiences.sample(min(batch_size, experiences.size()))\n",
    "    with torch.no_grad():\n",
    "        _, nextStateLogProbs, nextStateActionprobs = actor.get_action(sampledExperiences.next_observations)\n",
    "        QFunction1NextTarget = QNet1_target(sampledExperiences.next_observations)\n",
    "        QFunction2NextTarget = QNet2_target(sampledExperiences.next_observations)\n",
    "        minQNextTarget = nextStateActionprobs * (torch.min(QFunction1NextTarget, QFunction2NextTarget) - alpha * nextStateLogProbs)\n",
    "        minQNextTarget = minQNextTarget.sum(dim=1)\n",
    "        nextQValue = sampledExperiences.rewards.flatten() + (1 - sampledExperiences.dones.flatten()) * gamma * (minQNextTarget)\n",
    "\n",
    "    # use Q-values only for the taken actions\n",
    "    QFunction1ActionValues = QNet1(sampledExperiences.observations).gather(1, sampledExperiences.actions.long()).view(-1)\n",
    "    QFunction2ActionValues = QNet2(sampledExperiences.observations).gather(1, sampledExperiences.actions.long()).view(-1)\n",
    "    QFunction1Loss = F.mse_loss(QFunction1ActionValues, nextQValue)\n",
    "    QFunction2Loss = F.mse_loss(QFunction2ActionValues, nextQValue)\n",
    "    QFunctionsTotalLoss = QFunction1Loss + QFunction2Loss\n",
    "\n",
    "    QNetsOptimizer.zero_grad()\n",
    "    QFunctionsTotalLoss.backward()\n",
    "    QNetsOptimizer.step()\n",
    "\n",
    "    if globalStep % policyFrequency == 0:\n",
    "        for i in range(policyFrequency):\n",
    "            if i > 0:\n",
    "                # Sample new experiences if we make multiple updates\n",
    "                sampledExperiences = experiences.sample(min(batch_size, experiences.size()))\n",
    "\n",
    "            _, logProbabilities, actionProbabilities = actor.get_action(sampledExperiences.observations)\n",
    "            with torch.no_grad():\n",
    "                    batchStateValues1 = QNet1(sampledExperiences.observations)\n",
    "                    batchStateValues2 = QNet2(sampledExperiences.observations)\n",
    "                    batchStateValues = torch.min(batchStateValues1, batchStateValues2)\n",
    "\n",
    "            actorLoss = (actionProbabilities * (alpha * logProbabilities) - batchStateValues).mean()\n",
    "\n",
    "            actorOptimizer.zero_grad()\n",
    "            actorLoss.backward()\n",
    "            actorOptimizer.step()\n",
    "\n",
    "            if autoEntropy:\n",
    "                with torch.no_grad():\n",
    "                    _, logProbabilities, _ = actor.get_action(sampledExperiences.observations)\n",
    "                alphaLoss = (-logAlpha.exp()*(logProbabilities + targetEntropy)).mean()\n",
    "\n",
    "                alphaOptimizer.zero_grad()\n",
    "                alphaLoss.backward()\n",
    "                alphaOptimizer.step()\n",
    "                alpha = logAlpha.exp().item()\n",
    "\n",
    "    if globalStep % QNetworkFrequency == 0:\n",
    "        for param, targetParam in zip(QNet1.parameters(), QNet1_target.parameters()):\n",
    "            targetParam.data.copy_(tau*param.data + (1 - tau)*targetParam.data)\n",
    "        for param, targetParam in zip(QNet2.parameters(), QNet2_target.parameters()):\n",
    "            targetParam.data.copy_(tau*param.data + (1 - tau)*targetParam.data)\n",
    "envs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
