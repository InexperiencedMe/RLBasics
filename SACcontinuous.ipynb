{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "torch.set_printoptions(linewidth=120, precision=2, sci_mode=False, profile=\"short\")\n",
    "import cv2 as cv\n",
    "\n",
    "# Doesnt support more than 1 env yet\n",
    "# I dont think sampledExperienced.dones works at all, where do the names come from? Remake it with namedTuple\n",
    "\n",
    "seed = 1\n",
    "nEnvs = 1\n",
    "torch_deterministic = True\n",
    "env_id = \"LunarLanderContinuous-v2\"\n",
    "totalTimesteps = 10000\n",
    "buffer_size = int(1e6)\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 256\n",
    "policy_lr = 3e-4\n",
    "q_lr = 1e-3\n",
    "policyFrequency = 2\n",
    "QNetworkFrequency = 1  # Denis Yarats' implementation delays this by 2.\n",
    "noise_clip = 0.5\n",
    "alpha = 0.2\n",
    "autoEntropy = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "envs.single_action_space.shape = (2,). Prod = 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = x.to(torch.float)\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = torch.clamp(self.fc_logstd(x), LOG_STD_MIN, LOG_STD_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x, evaluation=False):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        # print(f\"mean, std shapes: {mean.shape}, {std.shape}\")\n",
    "        policyDistribution = torch.distributions.Normal(mean, std)\n",
    "        if evaluation:\n",
    "            actionSample = mean\n",
    "        else:\n",
    "            actionSample = policyDistribution.rsample()\n",
    "        # print(f\"actionSample.shape {actionSample.shape}\")\n",
    "        tanhAction = torch.tanh(actionSample)\n",
    "        # print(f\"tanhAction.shape {tanhAction.shape}\")\n",
    "        action = tanhAction * self.action_scale + self.action_bias\n",
    "        # print(f\"action.shape {action.shape}\")\n",
    "        log_prob = policyDistribution.log_prob(actionSample)\n",
    "        # print(f\"log_prob.shape {log_prob.shape}, log_prob {log_prob}\")\n",
    "        # print(f\"tanhAction.shape {tanhAction.shape}, tanhAction {tanhAction}\")\n",
    "        # print(f\"action_scale.shape {self.action_scale.shape}, action_scale {self.action_scale}\")\n",
    "        # print(f\"Trying to add to log prob log of self.action_scale * (1 - tanhAction.pow(2)) + 1e-6: {self.action_scale * (1 - tanhAction.pow(2)) + 1e-6}\")\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - tanhAction.pow(2)) + 1e-6)\n",
    "        # print(f\"Added it, so now log_prob is {log_prob} of shape {log_prob.shape}\")\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        # print(f\"Summed it so now logprob is {log_prob} of shape {log_prob.shape}\")\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "envs = gym.vector.make(env_id, num_envs=nEnvs)\n",
    "envs.single_observation_space.dtype = np.float32\n",
    "totalEpisodicRewards = torch.zeros(nEnvs).to(device)\n",
    "print(f\"envs.single_action_space.shape = {envs.single_action_space.shape}. Prod = {np.prod(envs.single_action_space.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.9980071187019348\n",
      "Alpha: 0.9980071187019348\n",
      "Alpha: 0.9960227012634277\n",
      "Alpha: 0.9960227012634277\n",
      "Alpha: 0.9940404891967773\n",
      "Alpha: 0.9940404891967773\n",
      "Alpha: 0.9920586943626404\n",
      "Alpha: 0.9920586943626404\n",
      "Alpha: 0.9900829195976257\n",
      "Alpha: 0.9900829195976257\n",
      "Alpha: 0.9881163239479065\n",
      "Alpha: 0.9881163239479065\n",
      "Alpha: 0.986151397228241\n",
      "Alpha: 0.986151397228241\n",
      "Alpha: 0.9841977953910828\n",
      "Alpha: 0.9841977953910828\n",
      "Alpha: 0.9822391867637634\n",
      "Alpha: 0.9822391867637634\n",
      "Alpha: 0.9802781343460083\n",
      "Alpha: 0.9802781343460083\n",
      "Alpha: 0.9783176183700562\n",
      "Alpha: 0.9783176183700562\n",
      "Alpha: 0.9763534069061279\n",
      "Alpha: 0.9763534069061279\n",
      "Alpha: 0.9743953347206116\n",
      "Alpha: 0.9743953347206116\n",
      "Alpha: 0.9724488258361816\n",
      "Alpha: 0.9724488258361816\n",
      "Alpha: 0.9705116152763367\n",
      "Alpha: 0.9705116152763367\n",
      "Alpha: 0.9685928225517273\n",
      "Alpha: 0.9685928225517273\n",
      "Alpha: 0.9666793346405029\n",
      "Alpha: 0.9666793346405029\n",
      "Alpha: 0.9647737741470337\n",
      "Alpha: 0.9647737741470337\n",
      "Alpha: 0.9628947377204895\n",
      "Alpha: 0.9628947377204895\n",
      "Alpha: 0.9610310196876526\n",
      "Alpha: 0.9610310196876526\n",
      "Alpha: 0.9591860175132751\n",
      "Alpha: 0.9591860175132751\n",
      "Alpha: 0.9573574662208557\n",
      "Alpha: 0.9573574662208557\n",
      "Alpha: 0.9555490016937256\n",
      "Alpha: 0.9555490016937256\n",
      "Alpha: 0.9537575244903564\n",
      "Alpha: 0.9537575244903564\n",
      "Alpha: 0.9519780874252319\n",
      "Alpha: 0.9519780874252319\n",
      "Alpha: 0.9501957893371582\n",
      "Alpha: 0.9501957893371582\n",
      "Alpha: 0.9484068751335144\n",
      "Alpha: 0.9484068751335144\n",
      "Alpha: 0.9466056823730469\n",
      "Alpha: 0.9466056823730469\n",
      "Alpha: 0.944804847240448\n",
      "Alpha: 0.944804847240448\n",
      "Alpha: 0.9429998397827148\n",
      "Alpha: 0.9429998397827148\n",
      "Alpha: 0.941184937953949\n",
      "Alpha: 0.941184937953949\n",
      "Alpha: 0.9393665194511414\n",
      "Alpha: 0.9393665194511414\n",
      "Alpha: 0.9375370144844055\n",
      "Alpha: 0.9375370144844055\n",
      "Alpha: 0.9357013702392578\n",
      "Alpha: 0.9357013702392578\n",
      "Alpha: 0.9338632225990295\n",
      "Alpha: 0.9338632225990295\n",
      "Alpha: 0.932020366191864\n",
      "Alpha: 0.932020366191864\n",
      "Alpha: 0.930180549621582\n",
      "Alpha: 0.930180549621582\n",
      "Alpha: 0.9283407330513\n",
      "Alpha: 0.9283407330513\n",
      "Alpha: 0.9265016913414001\n",
      "Alpha: 0.9265016913414001\n",
      "Alpha: 0.9246652722358704\n",
      "Alpha: 0.9246652722358704\n",
      "Alpha: 0.9228297472000122\n",
      "Alpha: 0.9228297472000122\n",
      "Alpha: 0.9209984540939331\n",
      "Alpha: 0.9209984540939331\n",
      "Alpha: 0.9191702604293823\n",
      "Alpha: 0.9191702604293823\n",
      "Alpha: 0.9173504710197449\n",
      "Alpha: 0.9173504710197449\n",
      "Alpha: 0.9155400991439819\n",
      "Alpha: 0.9155400991439819\n",
      "Alpha: 0.9137365818023682\n",
      "Alpha: 0.9137365818023682\n",
      "Alpha: 0.9119425415992737\n",
      "Alpha: 0.9119425415992737\n",
      "Alpha: 0.9101529121398926\n",
      "Alpha: 0.9101529121398926\n",
      "Alpha: 0.9083725214004517\n",
      "Alpha: 0.9083725214004517\n",
      "Alpha: 0.9066041111946106\n",
      "Alpha: 0.9066041111946106\n",
      "Alpha: 0.9048472046852112\n",
      "Alpha: 0.9048472046852112\n",
      "Alpha: 0.903095543384552\n",
      "Alpha: 0.903095543384552\n",
      "Alpha: 0.9013569355010986\n",
      "Alpha: 0.9013569355010986\n",
      "Alpha: 0.8996253609657288\n",
      "Alpha: 0.8996253609657288\n",
      "Alpha: 0.8979005217552185\n",
      "Alpha: 0.8979005217552185\n",
      "Alpha: 0.8961765170097351\n",
      "Alpha: 0.8961765170097351\n",
      "Alpha: 0.8944546580314636\n",
      "Alpha: 0.8944546580314636\n",
      "Alpha: 0.8927395939826965\n",
      "Alpha: 0.8927395939826965\n",
      "Alpha: 0.8910257816314697\n",
      "Score:  -280.52\n",
      "Alpha: 0.8910257816314697\n",
      "Alpha: 0.8893155455589294\n",
      "Alpha: 0.8893155455589294\n",
      "Alpha: 0.8876107335090637\n",
      "Alpha: 0.8876107335090637\n",
      "Alpha: 0.8859092593193054\n",
      "Alpha: 0.8859092593193054\n",
      "Alpha: 0.8842093348503113\n",
      "Alpha: 0.8842093348503113\n",
      "Alpha: 0.8825165033340454\n",
      "Alpha: 0.8825165033340454\n",
      "Alpha: 0.8808265924453735\n",
      "Alpha: 0.8808265924453735\n",
      "Alpha: 0.8791391253471375\n",
      "Alpha: 0.8791391253471375\n",
      "Alpha: 0.8774589896202087\n",
      "Alpha: 0.8774589896202087\n",
      "Alpha: 0.8757933378219604\n",
      "Alpha: 0.8757933378219604\n",
      "Alpha: 0.8741410970687866\n",
      "Alpha: 0.8741410970687866\n",
      "Alpha: 0.8725014328956604\n",
      "Alpha: 0.8725014328956604\n",
      "Alpha: 0.8708813190460205\n",
      "Alpha: 0.8708813190460205\n",
      "Alpha: 0.8692805171012878\n",
      "Alpha: 0.8692805171012878\n",
      "Alpha: 0.8676968216896057\n",
      "Alpha: 0.8676968216896057\n",
      "Alpha: 0.8661239743232727\n",
      "Alpha: 0.8661239743232727\n",
      "Alpha: 0.8645660877227783\n",
      "Alpha: 0.8645660877227783\n",
      "Alpha: 0.8630216121673584\n",
      "Alpha: 0.8630216121673584\n",
      "Alpha: 0.8614963889122009\n",
      "Alpha: 0.8614963889122009\n",
      "Alpha: 0.859980583190918\n",
      "Alpha: 0.859980583190918\n",
      "Alpha: 0.8584756851196289\n",
      "Alpha: 0.8584756851196289\n",
      "Alpha: 0.856983482837677\n",
      "Alpha: 0.856983482837677\n",
      "Alpha: 0.8555071353912354\n",
      "Alpha: 0.8555071353912354\n",
      "Alpha: 0.8540380597114563\n",
      "Alpha: 0.8540380597114563\n",
      "Alpha: 0.8525661826133728\n",
      "Alpha: 0.8525661826133728\n",
      "Alpha: 0.8510916233062744\n",
      "Alpha: 0.8510916233062744\n",
      "Alpha: 0.8496129512786865\n",
      "Alpha: 0.8496129512786865\n",
      "Alpha: 0.8481259942054749\n",
      "Alpha: 0.8481259942054749\n",
      "Alpha: 0.8466297388076782\n",
      "Alpha: 0.8466297388076782\n",
      "Alpha: 0.8451285362243652\n",
      "Alpha: 0.8451285362243652\n",
      "Alpha: 0.8436212539672852\n",
      "Alpha: 0.8436212539672852\n",
      "Alpha: 0.8421066403388977\n",
      "Alpha: 0.8421066403388977\n",
      "Alpha: 0.8405883312225342\n",
      "Alpha: 0.8405883312225342\n",
      "Alpha: 0.8390706181526184\n",
      "Alpha: 0.8390706181526184\n",
      "Alpha: 0.8375552892684937\n",
      "Alpha: 0.8375552892684937\n",
      "Alpha: 0.8360428214073181\n",
      "Alpha: 0.8360428214073181\n",
      "Alpha: 0.8345376253128052\n",
      "Alpha: 0.8345376253128052\n",
      "Alpha: 0.8330405354499817\n",
      "Alpha: 0.8330405354499817\n",
      "Alpha: 0.8315544724464417\n",
      "Alpha: 0.8315544724464417\n",
      "Alpha: 0.8300804495811462\n",
      "Alpha: 0.8300804495811462\n",
      "Alpha: 0.8286173343658447\n",
      "Alpha: 0.8286173343658447\n",
      "Alpha: 0.8271659016609192\n",
      "Alpha: 0.8271659016609192\n",
      "Alpha: 0.8257196545600891\n",
      "Alpha: 0.8257196545600891\n",
      "Alpha: 0.824277937412262\n",
      "Alpha: 0.824277937412262\n",
      "Alpha: 0.8228415846824646\n",
      "Alpha: 0.8228415846824646\n",
      "Alpha: 0.821411669254303\n",
      "Alpha: 0.821411669254303\n",
      "Alpha: 0.8199854493141174\n",
      "Alpha: 0.8199854493141174\n",
      "Alpha: 0.8185616135597229\n",
      "Alpha: 0.8185616135597229\n",
      "Alpha: 0.8171481490135193\n",
      "Alpha: 0.8171481490135193\n",
      "Score:   -95.80\n",
      "Alpha: 0.8157443404197693\n",
      "Alpha: 0.8157443404197693\n",
      "Alpha: 0.8143530488014221\n",
      "Alpha: 0.8143530488014221\n",
      "Alpha: 0.812975287437439\n",
      "Alpha: 0.812975287437439\n",
      "Alpha: 0.8116095662117004\n",
      "Alpha: 0.8116095662117004\n",
      "Alpha: 0.810250997543335\n",
      "Alpha: 0.810250997543335\n",
      "Alpha: 0.8088976144790649\n",
      "Alpha: 0.8088976144790649\n",
      "Alpha: 0.8075529932975769\n",
      "Alpha: 0.8075529932975769\n",
      "Alpha: 0.8062175512313843\n",
      "Alpha: 0.8062175512313843\n",
      "Alpha: 0.8048992156982422\n",
      "Alpha: 0.8048992156982422\n",
      "Alpha: 0.8035971522331238\n",
      "Alpha: 0.8035971522331238\n",
      "Alpha: 0.8023113012313843\n",
      "Alpha: 0.8023113012313843\n",
      "Alpha: 0.8010432720184326\n",
      "Alpha: 0.8010432720184326\n",
      "Alpha: 0.7997995615005493\n",
      "Alpha: 0.7997995615005493\n",
      "Alpha: 0.7985785007476807\n",
      "Alpha: 0.7985785007476807\n",
      "Alpha: 0.797379195690155\n",
      "Alpha: 0.797379195690155\n",
      "Alpha: 0.7961937189102173\n",
      "Alpha: 0.7961937189102173\n",
      "Alpha: 0.7950121164321899\n",
      "Alpha: 0.7950121164321899\n",
      "Alpha: 0.7938265204429626\n",
      "Alpha: 0.7938265204429626\n",
      "Alpha: 0.7926315665245056\n",
      "Alpha: 0.7926315665245056\n",
      "Alpha: 0.7914327383041382\n",
      "Alpha: 0.7914327383041382\n",
      "Alpha: 0.7902312278747559\n",
      "Alpha: 0.7902312278747559\n",
      "Alpha: 0.7890231609344482\n",
      "Alpha: 0.7890231609344482\n",
      "Alpha: 0.7878033518791199\n",
      "Alpha: 0.7878033518791199\n",
      "Alpha: 0.7865779995918274\n",
      "Alpha: 0.7865779995918274\n",
      "Alpha: 0.785343587398529\n",
      "Alpha: 0.785343587398529\n",
      "Alpha: 0.7841096520423889\n",
      "Alpha: 0.7841096520423889\n",
      "Alpha: 0.7828750014305115\n",
      "Alpha: 0.7828750014305115\n",
      "Alpha: 0.7816398739814758\n",
      "Alpha: 0.7816398739814758\n",
      "Alpha: 0.7804110646247864\n",
      "Alpha: 0.7804110646247864\n",
      "Alpha: 0.7791896462440491\n",
      "Alpha: 0.7791896462440491\n",
      "Alpha: 0.7779710292816162\n",
      "Alpha: 0.7779710292816162\n",
      "Alpha: 0.7767560482025146\n",
      "Alpha: 0.7767560482025146\n",
      "Alpha: 0.7755353450775146\n",
      "Alpha: 0.7755353450775146\n",
      "Alpha: 0.774320662021637\n",
      "Alpha: 0.774320662021637\n",
      "Alpha: 0.773099958896637\n",
      "Alpha: 0.773099958896637\n",
      "Alpha: 0.7718761563301086\n",
      "Alpha: 0.7718761563301086\n",
      "Alpha: 0.7706478238105774\n",
      "Alpha: 0.7706478238105774\n",
      "Alpha: 0.7694194316864014\n",
      "Alpha: 0.7694194316864014\n",
      "Alpha: 0.7681851387023926\n",
      "Alpha: 0.7681851387023926\n",
      "Alpha: 0.7669538259506226\n",
      "Alpha: 0.7669538259506226\n",
      "Alpha: 0.7657387852668762\n",
      "Alpha: 0.7657387852668762\n",
      "Alpha: 0.7645352482795715\n",
      "Alpha: 0.7645352482795715\n",
      "Alpha: 0.7633441090583801\n",
      "Alpha: 0.7633441090583801\n",
      "Alpha: 0.7621655464172363\n",
      "Alpha: 0.7621655464172363\n",
      "Alpha: 0.7609866857528687\n",
      "Alpha: 0.7609866857528687\n",
      "Alpha: 0.759800910949707\n",
      "Alpha: 0.759800910949707\n",
      "Alpha: 0.7586149573326111\n",
      "Alpha: 0.7586149573326111\n",
      "Alpha: 0.7574247121810913\n",
      "Alpha: 0.7574247121810913\n",
      "Alpha: 0.7562392950057983\n",
      "Alpha: 0.7562392950057983\n",
      "Alpha: 0.7550596594810486\n",
      "Alpha: 0.7550596594810486\n",
      "Alpha: 0.7538878917694092\n",
      "Alpha: 0.7538878917694092\n",
      "Alpha: 0.7527231574058533\n",
      "Alpha: 0.7527231574058533\n",
      "Alpha: 0.7515672445297241\n",
      "Alpha: 0.7515672445297241\n",
      "Alpha: 0.7504138350486755\n",
      "Alpha: 0.7504138350486755\n",
      "Alpha: 0.7492635846138\n",
      "Alpha: 0.7492635846138\n",
      "Alpha: 0.7481089234352112\n",
      "Alpha: 0.7481089234352112\n",
      "Alpha: 0.7469578981399536\n",
      "Alpha: 0.7469578981399536\n",
      "Alpha: 0.7458025217056274\n",
      "Alpha: 0.7458025217056274\n",
      "Alpha: 0.7446473240852356\n",
      "Alpha: 0.7446473240852356\n",
      "Alpha: 0.7434921860694885\n",
      "Alpha: 0.7434921860694885\n",
      "Alpha: 0.7423385381698608\n",
      "Alpha: 0.7423385381698608\n",
      "Alpha: 0.741189181804657\n",
      "Alpha: 0.741189181804657\n",
      "Alpha: 0.7400504946708679\n",
      "Alpha: 0.7400504946708679\n",
      "Alpha: 0.7389234900474548\n",
      "Alpha: 0.7389234900474548\n",
      "Alpha: 0.7378088235855103\n",
      "Alpha: 0.7378088235855103\n",
      "Alpha: 0.7366991639137268\n",
      "Alpha: 0.7366991639137268\n",
      "Alpha: 0.7355943918228149\n",
      "Alpha: 0.7355943918228149\n",
      "Alpha: 0.7344808578491211\n",
      "Alpha: 0.7344808578491211\n",
      "Alpha: 0.7333561778068542\n",
      "Alpha: 0.7333561778068542\n",
      "Alpha: 0.7322232723236084\n",
      "Alpha: 0.7322232723236084\n",
      "Alpha: 0.7310807704925537\n",
      "Alpha: 0.7310807704925537\n",
      "Alpha: 0.7299328446388245\n",
      "Alpha: 0.7299328446388245\n",
      "Alpha: 0.7287801504135132\n",
      "Alpha: 0.7287801504135132\n",
      "Alpha: 0.7276297807693481\n",
      "Alpha: 0.7276297807693481\n",
      "Alpha: 0.7264872193336487\n",
      "Alpha: 0.7264872193336487\n",
      "Alpha: 0.7253552675247192\n",
      "Alpha: 0.7253552675247192\n",
      "Alpha: 0.7242246270179749\n",
      "Alpha: 0.7242246270179749\n",
      "Score:  -706.47\n",
      "Alpha: 0.7230942249298096\n",
      "Alpha: 0.7230942249298096\n",
      "Alpha: 0.7219642996788025\n",
      "Alpha: 0.7219642996788025\n",
      "Alpha: 0.7208333611488342\n",
      "Alpha: 0.7208333611488342\n",
      "Alpha: 0.7197046279907227\n",
      "Alpha: 0.7197046279907227\n",
      "Alpha: 0.7185806632041931\n",
      "Alpha: 0.7185806632041931\n",
      "Alpha: 0.7174630165100098\n",
      "Alpha: 0.7174630165100098\n",
      "Alpha: 0.7163577675819397\n",
      "Alpha: 0.7163577675819397\n",
      "Alpha: 0.7152714133262634\n",
      "Alpha: 0.7152714133262634\n",
      "Alpha: 0.7142027020454407\n",
      "Alpha: 0.7142027020454407\n",
      "Alpha: 0.7131417989730835\n",
      "Alpha: 0.7131417989730835\n",
      "Alpha: 0.712083101272583\n",
      "Alpha: 0.712083101272583\n",
      "Alpha: 0.7110185027122498\n",
      "Alpha: 0.7110185027122498\n",
      "Alpha: 0.7099506855010986\n",
      "Alpha: 0.7099506855010986\n",
      "Alpha: 0.7088825702667236\n",
      "Alpha: 0.7088825702667236\n",
      "Alpha: 0.7078194618225098\n",
      "Alpha: 0.7078194618225098\n",
      "Alpha: 0.7067628502845764\n",
      "Alpha: 0.7067628502845764\n",
      "Alpha: 0.705704927444458\n",
      "Alpha: 0.705704927444458\n",
      "Alpha: 0.7046440839767456\n",
      "Alpha: 0.7046440839767456\n",
      "Alpha: 0.7035793662071228\n",
      "Alpha: 0.7035793662071228\n",
      "Alpha: 0.7025086879730225\n",
      "Alpha: 0.7025086879730225\n",
      "Alpha: 0.7014349102973938\n",
      "Alpha: 0.7014349102973938\n",
      "Alpha: 0.7003560662269592\n",
      "Alpha: 0.7003560662269592\n",
      "Alpha: 0.6992740631103516\n",
      "Alpha: 0.6992740631103516\n",
      "Alpha: 0.6981895565986633\n",
      "Alpha: 0.6981895565986633\n",
      "Alpha: 0.6971023082733154\n",
      "Alpha: 0.6971023082733154\n",
      "Alpha: 0.6960174441337585\n",
      "Alpha: 0.6960174441337585\n",
      "Alpha: 0.694935142993927\n",
      "Alpha: 0.694935142993927\n",
      "Alpha: 0.6938494443893433\n",
      "Alpha: 0.6938494443893433\n",
      "Alpha: 0.6927610635757446\n",
      "Alpha: 0.6927610635757446\n",
      "Alpha: 0.6916754841804504\n",
      "Alpha: 0.6916754841804504\n",
      "Alpha: 0.690585196018219\n",
      "Alpha: 0.690585196018219\n",
      "Alpha: 0.6894914507865906\n",
      "Alpha: 0.6894914507865906\n",
      "Alpha: 0.6883967518806458\n",
      "Alpha: 0.6883967518806458\n",
      "Alpha: 0.6873103380203247\n",
      "Alpha: 0.6873103380203247\n",
      "Alpha: 0.6862313151359558\n",
      "Alpha: 0.6862313151359558\n",
      "Alpha: 0.6851564049720764\n",
      "Alpha: 0.6851564049720764\n",
      "Alpha: 0.6840887069702148\n",
      "Alpha: 0.6840887069702148\n",
      "Alpha: 0.68302983045578\n",
      "Alpha: 0.68302983045578\n",
      "Alpha: 0.6819702982902527\n",
      "Alpha: 0.6819702982902527\n",
      "Alpha: 0.6809112429618835\n",
      "Alpha: 0.6809112429618835\n",
      "Alpha: 0.6798501014709473\n",
      "Alpha: 0.6798501014709473\n",
      "Alpha: 0.6787842512130737\n",
      "Alpha: 0.6787842512130737\n",
      "Alpha: 0.6777079105377197\n",
      "Alpha: 0.6777079105377197\n",
      "Alpha: 0.6766276359558105\n",
      "Alpha: 0.6766276359558105\n",
      "Alpha: 0.6755462288856506\n",
      "Alpha: 0.6755462288856506\n",
      "Alpha: 0.6744701266288757\n",
      "Alpha: 0.6744701266288757\n",
      "Alpha: 0.6734020113945007\n",
      "Alpha: 0.6734020113945007\n",
      "Alpha: 0.6723468899726868\n",
      "Alpha: 0.6723468899726868\n",
      "Alpha: 0.6713078618049622\n",
      "Alpha: 0.6713078618049622\n",
      "Alpha: 0.6702749133110046\n",
      "Alpha: 0.6702749133110046\n",
      "Alpha: 0.6692488789558411\n",
      "Alpha: 0.6692488789558411\n",
      "Alpha: 0.6682257652282715\n",
      "Alpha: 0.6682257652282715\n",
      "Alpha: 0.667201042175293\n",
      "Alpha: 0.667201042175293\n",
      "Alpha: 0.6661708950996399\n",
      "Alpha: 0.6661708950996399\n",
      "Alpha: 0.6651418805122375\n",
      "Alpha: 0.6651418805122375\n",
      "Alpha: 0.6641150116920471\n",
      "Alpha: 0.6641150116920471\n",
      "Alpha: 0.6630898118019104\n",
      "Alpha: 0.6630898118019104\n",
      "Alpha: 0.6620703935623169\n",
      "Alpha: 0.6620703935623169\n",
      "Alpha: 0.6610541343688965\n",
      "Alpha: 0.6610541343688965\n",
      "Alpha: 0.6600412130355835\n",
      "Alpha: 0.6600412130355835\n",
      "Alpha: 0.6590248346328735\n",
      "Alpha: 0.6590248346328735\n",
      "Alpha: 0.6580145955085754\n",
      "Alpha: 0.6580145955085754\n",
      "Alpha: 0.6570116877555847\n",
      "Alpha: 0.6570116877555847\n",
      "Alpha: 0.656013011932373\n",
      "Alpha: 0.656013011932373\n",
      "Alpha: 0.6550235152244568\n",
      "Alpha: 0.6550235152244568\n",
      "Alpha: 0.6540402173995972\n",
      "Alpha: 0.6540402173995972\n",
      "Alpha: 0.6530663371086121\n",
      "Alpha: 0.6530663371086121\n",
      "Alpha: 0.6520901322364807\n",
      "Alpha: 0.6520901322364807\n",
      "Alpha: 0.6511155366897583\n",
      "Alpha: 0.6511155366897583\n",
      "Alpha: 0.6501425504684448\n",
      "Alpha: 0.6501425504684448\n",
      "Alpha: 0.6491659283638\n",
      "Alpha: 0.6491659283638\n",
      "Alpha: 0.6481853127479553\n",
      "Alpha: 0.6481853127479553\n",
      "Alpha: 0.6472004652023315\n",
      "Alpha: 0.6472004652023315\n",
      "Alpha: 0.646215558052063\n",
      "Alpha: 0.646215558052063\n",
      "Alpha: 0.6452348828315735\n",
      "Alpha: 0.6452348828315735\n",
      "Alpha: 0.6442641615867615\n",
      "Alpha: 0.6442641615867615\n",
      "Alpha: 0.6432974338531494\n",
      "Alpha: 0.6432974338531494\n",
      "Alpha: 0.6423317790031433\n",
      "Alpha: 0.6423317790031433\n",
      "Alpha: 0.6413624882698059\n",
      "Alpha: 0.6413624882698059\n",
      "Alpha: 0.6403905153274536\n",
      "Alpha: 0.6403905153274536\n",
      "Score:  -578.20\n",
      "Alpha: 0.6394203901290894\n",
      "Alpha: 0.6394203901290894\n",
      "Alpha: 0.6384522914886475\n",
      "Alpha: 0.6384522914886475\n",
      "Alpha: 0.6374969482421875\n",
      "Alpha: 0.6374969482421875\n",
      "Alpha: 0.6365453004837036\n",
      "Alpha: 0.6365453004837036\n",
      "Alpha: 0.6355945467948914\n",
      "Alpha: 0.6355945467948914\n",
      "Alpha: 0.6346510648727417\n",
      "Alpha: 0.6346510648727417\n",
      "Alpha: 0.6337161064147949\n",
      "Alpha: 0.6337161064147949\n",
      "Alpha: 0.632793664932251\n",
      "Alpha: 0.632793664932251\n",
      "Alpha: 0.6318812966346741\n",
      "Alpha: 0.6318812966346741\n",
      "Alpha: 0.6309728622436523\n",
      "Alpha: 0.6309728622436523\n",
      "Alpha: 0.6300643682479858\n",
      "Alpha: 0.6300643682479858\n",
      "Alpha: 0.6291480660438538\n",
      "Alpha: 0.6291480660438538\n",
      "Alpha: 0.6282232403755188\n",
      "Alpha: 0.6282232403755188\n",
      "Alpha: 0.6272976398468018\n",
      "Alpha: 0.6272976398468018\n",
      "Alpha: 0.6263710260391235\n",
      "Alpha: 0.6263710260391235\n",
      "Alpha: 0.62544846534729\n",
      "Alpha: 0.62544846534729\n",
      "Alpha: 0.6245266199111938\n",
      "Alpha: 0.6245266199111938\n",
      "Alpha: 0.6236040592193604\n",
      "Alpha: 0.6236040592193604\n",
      "Alpha: 0.6226822137832642\n",
      "Alpha: 0.6226822137832642\n",
      "Alpha: 0.6217599511146545\n",
      "Alpha: 0.6217599511146545\n",
      "Alpha: 0.6208404302597046\n",
      "Alpha: 0.6208404302597046\n",
      "Alpha: 0.619931161403656\n",
      "Alpha: 0.619931161403656\n",
      "Alpha: 0.6190304756164551\n",
      "Alpha: 0.6190304756164551\n",
      "Alpha: 0.618137776851654\n",
      "Alpha: 0.618137776851654\n",
      "Alpha: 0.6172569394111633\n",
      "Alpha: 0.6172569394111633\n",
      "Alpha: 0.6163761019706726\n",
      "Alpha: 0.6163761019706726\n",
      "Alpha: 0.6154972910881042\n",
      "Alpha: 0.6154972910881042\n",
      "Alpha: 0.6146137118339539\n",
      "Alpha: 0.6146137118339539\n",
      "Alpha: 0.6137233376502991\n",
      "Alpha: 0.6137233376502991\n",
      "Alpha: 0.6128291487693787\n",
      "Alpha: 0.6128291487693787\n",
      "Alpha: 0.611937403678894\n",
      "Alpha: 0.611937403678894\n",
      "Alpha: 0.6110500693321228\n",
      "Alpha: 0.6110500693321228\n",
      "Alpha: 0.6101664304733276\n",
      "Alpha: 0.6101664304733276\n",
      "Alpha: 0.6092879176139832\n",
      "Alpha: 0.6092879176139832\n",
      "Alpha: 0.608414351940155\n",
      "Alpha: 0.608414351940155\n",
      "Alpha: 0.6075462698936462\n",
      "Alpha: 0.6075462698936462\n",
      "Alpha: 0.6066803932189941\n",
      "Alpha: 0.6066803932189941\n",
      "Alpha: 0.6058117747306824\n",
      "Alpha: 0.6058117747306824\n",
      "Alpha: 0.6049465537071228\n",
      "Alpha: 0.6049465537071228\n",
      "Alpha: 0.6040856838226318\n",
      "Alpha: 0.6040856838226318\n",
      "Alpha: 0.6032265424728394\n",
      "Alpha: 0.6032265424728394\n",
      "Alpha: 0.602371096611023\n",
      "Alpha: 0.602371096611023\n",
      "Alpha: 0.6015288829803467\n",
      "Alpha: 0.6015288829803467\n",
      "Alpha: 0.6006931662559509\n",
      "Alpha: 0.6006931662559509\n",
      "Alpha: 0.5998565554618835\n",
      "Alpha: 0.5998565554618835\n",
      "Alpha: 0.599015474319458\n",
      "Alpha: 0.599015474319458\n",
      "Alpha: 0.5981672406196594\n",
      "Alpha: 0.5981672406196594\n",
      "Alpha: 0.5973166823387146\n",
      "Alpha: 0.5973166823387146\n",
      "Alpha: 0.5964587926864624\n",
      "Alpha: 0.5964587926864624\n",
      "Alpha: 0.5955976247787476\n",
      "Alpha: 0.5955976247787476\n",
      "Alpha: 0.5947334170341492\n",
      "Alpha: 0.5947334170341492\n",
      "Alpha: 0.5938742160797119\n",
      "Alpha: 0.5938742160797119\n",
      "Alpha: 0.5930175185203552\n",
      "Alpha: 0.5930175185203552\n",
      "Alpha: 0.5921630263328552\n",
      "Alpha: 0.5921630263328552\n",
      "Alpha: 0.5913134813308716\n",
      "Alpha: 0.5913134813308716\n",
      "Alpha: 0.59046870470047\n",
      "Alpha: 0.59046870470047\n",
      "Alpha: 0.589629054069519\n",
      "Alpha: 0.589629054069519\n",
      "Alpha: 0.5887929201126099\n",
      "Alpha: 0.5887929201126099\n",
      "Alpha: 0.5879589319229126\n",
      "Alpha: 0.5879589319229126\n",
      "Alpha: 0.5871239900588989\n",
      "Alpha: 0.5871239900588989\n",
      "Alpha: 0.5862961411476135\n",
      "Alpha: 0.5862961411476135\n",
      "Alpha: 0.5854719877243042\n",
      "Alpha: 0.5854719877243042\n",
      "Alpha: 0.5846500396728516\n",
      "Alpha: 0.5846500396728516\n",
      "Alpha: 0.5838302969932556\n",
      "Alpha: 0.5838302969932556\n",
      "Alpha: 0.5830127000808716\n",
      "Alpha: 0.5830127000808716\n",
      "Alpha: 0.582190752029419\n",
      "Alpha: 0.582190752029419\n",
      "Alpha: 0.5813616514205933\n",
      "Alpha: 0.5813616514205933\n",
      "Alpha: 0.5805309414863586\n",
      "Alpha: 0.5805309414863586\n",
      "Alpha: 0.5796977281570435\n",
      "Alpha: 0.5796977281570435\n",
      "Alpha: 0.5788663029670715\n",
      "Alpha: 0.5788663029670715\n",
      "Score:  -522.99\n",
      "Alpha: 0.5780396461486816\n",
      "Alpha: 0.5780396461486816\n",
      "Alpha: 0.5772161483764648\n",
      "Alpha: 0.5772161483764648\n",
      "Alpha: 0.5764013528823853\n",
      "Alpha: 0.5764013528823853\n",
      "Alpha: 0.5755967497825623\n",
      "Alpha: 0.5755967497825623\n",
      "Alpha: 0.5747948288917542\n",
      "Alpha: 0.5747948288917542\n",
      "Alpha: 0.5739951133728027\n",
      "Alpha: 0.5739951133728027\n",
      "Alpha: 0.5732012987136841\n",
      "Alpha: 0.5732012987136841\n",
      "Alpha: 0.5724098682403564\n",
      "Alpha: 0.5724098682403564\n",
      "Alpha: 0.571620523929596\n",
      "Alpha: 0.571620523929596\n",
      "Alpha: 0.5708317756652832\n",
      "Alpha: 0.5708317756652832\n",
      "Alpha: 0.5700437426567078\n",
      "Alpha: 0.5700437426567078\n",
      "Alpha: 0.5692570209503174\n",
      "Alpha: 0.5692570209503174\n",
      "Alpha: 0.5684677362442017\n",
      "Alpha: 0.5684677362442017\n",
      "Alpha: 0.5676807165145874\n",
      "Alpha: 0.5676807165145874\n",
      "Alpha: 0.5668932199478149\n",
      "Alpha: 0.5668932199478149\n",
      "Alpha: 0.5661028027534485\n",
      "Alpha: 0.5661028027534485\n",
      "Alpha: 0.5653093457221985\n",
      "Alpha: 0.5653093457221985\n",
      "Alpha: 0.5645119547843933\n",
      "Alpha: 0.5645119547843933\n",
      "Alpha: 0.5637163519859314\n",
      "Alpha: 0.5637163519859314\n",
      "Alpha: 0.5629281401634216\n",
      "Alpha: 0.5629281401634216\n",
      "Alpha: 0.5621466040611267\n",
      "Alpha: 0.5621466040611267\n",
      "Alpha: 0.5613683462142944\n",
      "Alpha: 0.5613683462142944\n",
      "Alpha: 0.560589075088501\n",
      "Alpha: 0.560589075088501\n",
      "Alpha: 0.5598133206367493\n",
      "Alpha: 0.5598133206367493\n",
      "Alpha: 0.5590417981147766\n",
      "Alpha: 0.5590417981147766\n",
      "Alpha: 0.5582675337791443\n",
      "Alpha: 0.5582675337791443\n",
      "Alpha: 0.5574945211410522\n",
      "Alpha: 0.5574945211410522\n",
      "Alpha: 0.5567189455032349\n",
      "Alpha: 0.5567189455032349\n",
      "Alpha: 0.555944561958313\n",
      "Alpha: 0.555944561958313\n",
      "Alpha: 0.5551714897155762\n",
      "Alpha: 0.5551714897155762\n",
      "Alpha: 0.5544029474258423\n",
      "Alpha: 0.5544029474258423\n",
      "Alpha: 0.5536379218101501\n",
      "Alpha: 0.5536379218101501\n",
      "Alpha: 0.5528783798217773\n",
      "Alpha: 0.5528783798217773\n",
      "Alpha: 0.5521201491355896\n",
      "Alpha: 0.5521201491355896\n",
      "Alpha: 0.5513635277748108\n",
      "Alpha: 0.5513635277748108\n",
      "Alpha: 0.5506092309951782\n",
      "Alpha: 0.5506092309951782\n",
      "Alpha: 0.5498558878898621\n",
      "Alpha: 0.5498558878898621\n",
      "Alpha: 0.5491020083427429\n",
      "Alpha: 0.5491020083427429\n",
      "Alpha: 0.5483473539352417\n",
      "Alpha: 0.5483473539352417\n",
      "Alpha: 0.54759681224823\n",
      "Alpha: 0.54759681224823\n",
      "Alpha: 0.5468474626541138\n",
      "Alpha: 0.5468474626541138\n",
      "Alpha: 0.5461024641990662\n",
      "Alpha: 0.5461024641990662\n",
      "Alpha: 0.5453612208366394\n",
      "Alpha: 0.5453612208366394\n",
      "Alpha: 0.5446237325668335\n",
      "Alpha: 0.5446237325668335\n",
      "Alpha: 0.5438900589942932\n",
      "Alpha: 0.5438900589942932\n",
      "Alpha: 0.5431587100028992\n",
      "Alpha: 0.5431587100028992\n",
      "Alpha: 0.5424296259880066\n",
      "Alpha: 0.5424296259880066\n",
      "Alpha: 0.54170161485672\n",
      "Alpha: 0.54170161485672\n",
      "Alpha: 0.5409715175628662\n",
      "Alpha: 0.5409715175628662\n",
      "Alpha: 0.5402417182922363\n",
      "Alpha: 0.5402417182922363\n",
      "Alpha: 0.5395132303237915\n",
      "Alpha: 0.5395132303237915\n",
      "Alpha: 0.538784384727478\n",
      "Alpha: 0.538784384727478\n",
      "Alpha: 0.5380579233169556\n",
      "Alpha: 0.5380579233169556\n",
      "Alpha: 0.5373333692550659\n",
      "Alpha: 0.5373333692550659\n",
      "Alpha: 0.5366079807281494\n",
      "Alpha: 0.5366079807281494\n",
      "Alpha: 0.5358855724334717\n",
      "Alpha: 0.5358855724334717\n",
      "Alpha: 0.5351639986038208\n",
      "Alpha: 0.5351639986038208\n",
      "Alpha: 0.534446656703949\n",
      "Alpha: 0.534446656703949\n",
      "Alpha: 0.5337334871292114\n",
      "Alpha: 0.5337334871292114\n",
      "Alpha: 0.5330233573913574\n",
      "Alpha: 0.5330233573913574\n",
      "Alpha: 0.5323144793510437\n",
      "Alpha: 0.5323144793510437\n",
      "Alpha: 0.5316078662872314\n",
      "Alpha: 0.5316078662872314\n",
      "Alpha: 0.5309051275253296\n",
      "Alpha: 0.5309051275253296\n",
      "Alpha: 0.5302082896232605\n",
      "Alpha: 0.5302082896232605\n",
      "Alpha: 0.5295180678367615\n",
      "Alpha: 0.5295180678367615\n",
      "Alpha: 0.5288324356079102\n",
      "Score:  -135.29\n",
      "Alpha: 0.5288324356079102\n",
      "Alpha: 0.5281440019607544\n",
      "Alpha: 0.5281440019607544\n",
      "Alpha: 0.5274534225463867\n",
      "Alpha: 0.5274534225463867\n",
      "Alpha: 0.5267660021781921\n",
      "Alpha: 0.5267660021781921\n",
      "Alpha: 0.5260809659957886\n",
      "Alpha: 0.5260809659957886\n",
      "Alpha: 0.5253976583480835\n",
      "Alpha: 0.5253976583480835\n",
      "Alpha: 0.5247165560722351\n",
      "Alpha: 0.5247165560722351\n",
      "Alpha: 0.5240402817726135\n",
      "Alpha: 0.5240402817726135\n",
      "Alpha: 0.5233699679374695\n",
      "Alpha: 0.5233699679374695\n",
      "Alpha: 0.522709310054779\n",
      "Alpha: 0.522709310054779\n",
      "Alpha: 0.5220557451248169\n",
      "Alpha: 0.5220557451248169\n",
      "Alpha: 0.5214108228683472\n",
      "Alpha: 0.5214108228683472\n",
      "Alpha: 0.5207705497741699\n",
      "Alpha: 0.5207705497741699\n",
      "Alpha: 0.5201337933540344\n",
      "Alpha: 0.5201337933540344\n",
      "Alpha: 0.51949542760849\n",
      "Alpha: 0.51949542760849\n",
      "Alpha: 0.5188553929328918\n",
      "Alpha: 0.5188553929328918\n",
      "Alpha: 0.5182081460952759\n",
      "Alpha: 0.5182081460952759\n",
      "Alpha: 0.5175524353981018\n",
      "Alpha: 0.5175524353981018\n",
      "Alpha: 0.5168926119804382\n",
      "Alpha: 0.5168926119804382\n",
      "Alpha: 0.5162354111671448\n",
      "Alpha: 0.5162354111671448\n",
      "Alpha: 0.5155821442604065\n",
      "Alpha: 0.5155821442604065\n",
      "Alpha: 0.514934778213501\n",
      "Alpha: 0.514934778213501\n",
      "Alpha: 0.5142998695373535\n",
      "Alpha: 0.5142998695373535\n",
      "Alpha: 0.5136731266975403\n",
      "Alpha: 0.5136731266975403\n",
      "Alpha: 0.5130479335784912\n",
      "Alpha: 0.5130479335784912\n",
      "Alpha: 0.5124247074127197\n",
      "Alpha: 0.5124247074127197\n",
      "Alpha: 0.5118035078048706\n",
      "Alpha: 0.5118035078048706\n",
      "Alpha: 0.5111837983131409\n",
      "Alpha: 0.5111837983131409\n",
      "Alpha: 0.510562002658844\n",
      "Alpha: 0.510562002658844\n",
      "Alpha: 0.5099325180053711\n",
      "Alpha: 0.5099325180053711\n",
      "Alpha: 0.5092945098876953\n",
      "Alpha: 0.5092945098876953\n",
      "Alpha: 0.5086496472358704\n",
      "Alpha: 0.5086496472358704\n",
      "Alpha: 0.50799959897995\n",
      "Alpha: 0.50799959897995\n",
      "Alpha: 0.5073479413986206\n",
      "Alpha: 0.5073479413986206\n",
      "Alpha: 0.5066992044448853\n",
      "Alpha: 0.5066992044448853\n",
      "Alpha: 0.5060545206069946\n",
      "Alpha: 0.5060545206069946\n",
      "Alpha: 0.5054154992103577\n",
      "Alpha: 0.5054154992103577\n",
      "Alpha: 0.5047821402549744\n",
      "Alpha: 0.5047821402549744\n",
      "Alpha: 0.5041606426239014\n",
      "Alpha: 0.5041606426239014\n",
      "Alpha: 0.5035456418991089\n",
      "Alpha: 0.5035456418991089\n",
      "Alpha: 0.5029348731040955\n",
      "Alpha: 0.5029348731040955\n",
      "Alpha: 0.5023254752159119\n",
      "Alpha: 0.5023254752159119\n",
      "Alpha: 0.5017186403274536\n",
      "Alpha: 0.5017186403274536\n",
      "Alpha: 0.5011172890663147\n",
      "Alpha: 0.5011172890663147\n",
      "Alpha: 0.5005187392234802\n",
      "Alpha: 0.5005187392234802\n",
      "Alpha: 0.49992111325263977\n",
      "Alpha: 0.49992111325263977\n",
      "Alpha: 0.4993249773979187\n",
      "Alpha: 0.4993249773979187\n",
      "Alpha: 0.49873074889183044\n",
      "Alpha: 0.49873074889183044\n",
      "Alpha: 0.49813520908355713\n",
      "Alpha: 0.49813520908355713\n",
      "Alpha: 0.4975350797176361\n",
      "Alpha: 0.4975350797176361\n",
      "Alpha: 0.4969315528869629\n",
      "Alpha: 0.4969315528869629\n",
      "Alpha: 0.4963231384754181\n",
      "Alpha: 0.4963231384754181\n",
      "Alpha: 0.4957163631916046\n",
      "Alpha: 0.4957163631916046\n",
      "Alpha: 0.49510928988456726\n",
      "Alpha: 0.49510928988456726\n",
      "Alpha: 0.4945089519023895\n",
      "Alpha: 0.4945089519023895\n",
      "Alpha: 0.4939146041870117\n",
      "Alpha: 0.4939146041870117\n",
      "Alpha: 0.4933265745639801\n",
      "Alpha: 0.4933265745639801\n",
      "Alpha: 0.49274149537086487\n",
      "Alpha: 0.49274149537086487\n",
      "Alpha: 0.4921577274799347\n",
      "Alpha: 0.4921577274799347\n",
      "Alpha: 0.49158164858818054\n",
      "Alpha: 0.49158164858818054\n",
      "Alpha: 0.4910033643245697\n",
      "Alpha: 0.4910033643245697\n",
      "Alpha: 0.49042147397994995\n",
      "Alpha: 0.49042147397994995\n",
      "Alpha: 0.48983505368232727\n",
      "Alpha: 0.48983505368232727\n",
      "Alpha: 0.4892456531524658\n",
      "Alpha: 0.4892456531524658\n",
      "Alpha: 0.4886500835418701\n",
      "Alpha: 0.4886500835418701\n",
      "Alpha: 0.48805204033851624\n",
      "Alpha: 0.48805204033851624\n",
      "Alpha: 0.4874545931816101\n",
      "Alpha: 0.4874545931816101\n",
      "Alpha: 0.48685628175735474\n",
      "Alpha: 0.48685628175735474\n",
      "Alpha: 0.48625773191452026\n",
      "Alpha: 0.48625773191452026\n",
      "Alpha: 0.4856601059436798\n",
      "Alpha: 0.4856601059436798\n",
      "Alpha: 0.48506414890289307\n",
      "Alpha: 0.48506414890289307\n",
      "Alpha: 0.48447227478027344\n",
      "Alpha: 0.48447227478027344\n",
      "Alpha: 0.4838811755180359\n",
      "Alpha: 0.4838811755180359\n",
      "Alpha: 0.4832914471626282\n",
      "Alpha: 0.4832914471626282\n",
      "Alpha: 0.48270559310913086\n",
      "Alpha: 0.48270559310913086\n",
      "Alpha: 0.4821241796016693\n",
      "Alpha: 0.4821241796016693\n",
      "Alpha: 0.48155006766319275\n",
      "Alpha: 0.48155006766319275\n",
      "Alpha: 0.4809836149215698\n",
      "Alpha: 0.4809836149215698\n",
      "Alpha: 0.4804254174232483\n",
      "Alpha: 0.4804254174232483\n",
      "Alpha: 0.4798683226108551\n",
      "Alpha: 0.4798683226108551\n",
      "Alpha: 0.47931185364723206\n",
      "Alpha: 0.47931185364723206\n",
      "Alpha: 0.4787518084049225\n",
      "Alpha: 0.4787518084049225\n",
      "Alpha: 0.4781889319419861\n",
      "Alpha: 0.4781889319419861\n",
      "Alpha: 0.47762104868888855\n",
      "Alpha: 0.47762104868888855\n",
      "Alpha: 0.4770466983318329\n",
      "Alpha: 0.4770466983318329\n",
      "Alpha: 0.476470947265625\n",
      "Alpha: 0.476470947265625\n",
      "Alpha: 0.4758923649787903\n",
      "Alpha: 0.4758923649787903\n",
      "Alpha: 0.4753127098083496\n",
      "Alpha: 0.4753127098083496\n",
      "Alpha: 0.4747357666492462\n",
      "Alpha: 0.4747357666492462\n",
      "Alpha: 0.4741598069667816\n",
      "Alpha: 0.4741598069667816\n",
      "Alpha: 0.47358372807502747\n",
      "Alpha: 0.47358372807502747\n",
      "Alpha: 0.4730100929737091\n",
      "Alpha: 0.4730100929737091\n",
      "Alpha: 0.472440242767334\n",
      "Alpha: 0.472440242767334\n",
      "Alpha: 0.47187340259552\n",
      "Alpha: 0.47187340259552\n",
      "Alpha: 0.4713110625743866\n",
      "Alpha: 0.4713110625743866\n",
      "Alpha: 0.4707467257976532\n",
      "Alpha: 0.4707467257976532\n",
      "Alpha: 0.47018200159072876\n",
      "Alpha: 0.47018200159072876\n",
      "Alpha: 0.46961551904678345\n",
      "Alpha: 0.46961551904678345\n",
      "Alpha: 0.4690469801425934\n",
      "Alpha: 0.4690469801425934\n",
      "Alpha: 0.46847954392433167\n",
      "Alpha: 0.46847954392433167\n",
      "Alpha: 0.4679117798805237\n",
      "Alpha: 0.4679117798805237\n",
      "Alpha: 0.4673442542552948\n",
      "Alpha: 0.4673442542552948\n",
      "Alpha: 0.4667789340019226\n",
      "Alpha: 0.4667789340019226\n",
      "Alpha: 0.466220885515213\n",
      "Alpha: 0.466220885515213\n",
      "Alpha: 0.4656602740287781\n",
      "Alpha: 0.4656602740287781\n",
      "Alpha: 0.46510419249534607\n",
      "Alpha: 0.46510419249534607\n",
      "Alpha: 0.4645540714263916\n",
      "Alpha: 0.4645540714263916\n",
      "Alpha: 0.46400541067123413\n",
      "Alpha: 0.46400541067123413\n",
      "Alpha: 0.4634557366371155\n",
      "Alpha: 0.4634557366371155\n",
      "Alpha: 0.4629068374633789\n",
      "Alpha: 0.4629068374633789\n",
      "Alpha: 0.46235644817352295\n",
      "Alpha: 0.46235644817352295\n",
      "Alpha: 0.46180853247642517\n",
      "Alpha: 0.46180853247642517\n",
      "Alpha: 0.46126294136047363\n",
      "Alpha: 0.46126294136047363\n",
      "Alpha: 0.46071675419807434\n",
      "Alpha: 0.46071675419807434\n",
      "Alpha: 0.4601711928844452\n",
      "Alpha: 0.4601711928844452\n",
      "Alpha: 0.4596266448497772\n",
      "Alpha: 0.4596266448497772\n",
      "Alpha: 0.4590860605239868\n",
      "Alpha: 0.4590860605239868\n",
      "Alpha: 0.45854753255844116\n",
      "Alpha: 0.45854753255844116\n",
      "Alpha: 0.4580131471157074\n",
      "Alpha: 0.4580131471157074\n",
      "Alpha: 0.45747897028923035\n",
      "Alpha: 0.45747897028923035\n",
      "Alpha: 0.4569489359855652\n",
      "Alpha: 0.4569489359855652\n",
      "Alpha: 0.45642173290252686\n",
      "Alpha: 0.45642173290252686\n",
      "Alpha: 0.45589736104011536\n",
      "Alpha: 0.45589736104011536\n",
      "Alpha: 0.45537516474723816\n",
      "Alpha: 0.45537516474723816\n",
      "Alpha: 0.45485779643058777\n",
      "Alpha: 0.45485779643058777\n",
      "Alpha: 0.4543420970439911\n",
      "Alpha: 0.4543420970439911\n",
      "Alpha: 0.45383042097091675\n",
      "Alpha: 0.45383042097091675\n",
      "Alpha: 0.45331689715385437\n",
      "Score:  -158.90\n",
      "Alpha: 0.45331689715385437\n",
      "Alpha: 0.4528025686740875\n",
      "Alpha: 0.4528025686740875\n",
      "Alpha: 0.45228686928749084\n",
      "Alpha: 0.45228686928749084\n",
      "Alpha: 0.45176729559898376\n",
      "Alpha: 0.45176729559898376\n",
      "Alpha: 0.4512462019920349\n",
      "Alpha: 0.4512462019920349\n",
      "Alpha: 0.4507252275943756\n",
      "Alpha: 0.4507252275943756\n",
      "Alpha: 0.4502081573009491\n",
      "Alpha: 0.4502081573009491\n",
      "Alpha: 0.4496953785419464\n",
      "Alpha: 0.4496953785419464\n",
      "Alpha: 0.4491874873638153\n",
      "Alpha: 0.4491874873638153\n",
      "Alpha: 0.4486856162548065\n",
      "Alpha: 0.4486856162548065\n",
      "Alpha: 0.44819125533103943\n",
      "Alpha: 0.44819125533103943\n",
      "Alpha: 0.44770050048828125\n",
      "Alpha: 0.44770050048828125\n",
      "Alpha: 0.4472079277038574\n",
      "Alpha: 0.4472079277038574\n",
      "Alpha: 0.4467102885246277\n",
      "Alpha: 0.4467102885246277\n",
      "Alpha: 0.4462062418460846\n",
      "Alpha: 0.4462062418460846\n",
      "Alpha: 0.44569915533065796\n",
      "Alpha: 0.44569915533065796\n",
      "Alpha: 0.44518575072288513\n",
      "Alpha: 0.44518575072288513\n",
      "Alpha: 0.4446660578250885\n",
      "Alpha: 0.4446660578250885\n",
      "Alpha: 0.4441428780555725\n",
      "Alpha: 0.4441428780555725\n",
      "Alpha: 0.44361793994903564\n",
      "Alpha: 0.44361793994903564\n",
      "Alpha: 0.4430910646915436\n",
      "Alpha: 0.4430910646915436\n",
      "Alpha: 0.442564994096756\n",
      "Alpha: 0.442564994096756\n",
      "Alpha: 0.4420386850833893\n",
      "Alpha: 0.4420386850833893\n",
      "Alpha: 0.4415166676044464\n",
      "Alpha: 0.4415166676044464\n",
      "Alpha: 0.4410019814968109\n",
      "Alpha: 0.4410019814968109\n",
      "Alpha: 0.44048798084259033\n",
      "Alpha: 0.44048798084259033\n",
      "Alpha: 0.43997785449028015\n",
      "Alpha: 0.43997785449028015\n",
      "Alpha: 0.4394749402999878\n",
      "Alpha: 0.4394749402999878\n",
      "Alpha: 0.43897345662117004\n",
      "Alpha: 0.43897345662117004\n",
      "Alpha: 0.4384734630584717\n",
      "Alpha: 0.4384734630584717\n",
      "Alpha: 0.437974750995636\n",
      "Alpha: 0.437974750995636\n",
      "Alpha: 0.43747568130493164\n",
      "Alpha: 0.43747568130493164\n",
      "Alpha: 0.436977356672287\n",
      "Alpha: 0.436977356672287\n",
      "Alpha: 0.4364793002605438\n",
      "Alpha: 0.4364793002605438\n",
      "Alpha: 0.4359777569770813\n",
      "Alpha: 0.4359777569770813\n",
      "Alpha: 0.4354766309261322\n",
      "Alpha: 0.4354766309261322\n",
      "Alpha: 0.43497270345687866\n",
      "Alpha: 0.43497270345687866\n",
      "Alpha: 0.4344683587551117\n",
      "Alpha: 0.4344683587551117\n",
      "Alpha: 0.43396225571632385\n",
      "Alpha: 0.43396225571632385\n",
      "Alpha: 0.43345773220062256\n",
      "Alpha: 0.43345773220062256\n",
      "Alpha: 0.43295717239379883\n",
      "Alpha: 0.43295717239379883\n",
      "Alpha: 0.4324578642845154\n",
      "Alpha: 0.4324578642845154\n",
      "Alpha: 0.4319628179073334\n",
      "Alpha: 0.4319628179073334\n",
      "Alpha: 0.43147119879722595\n",
      "Alpha: 0.43147119879722595\n",
      "Alpha: 0.4309844970703125\n",
      "Alpha: 0.4309844970703125\n",
      "Alpha: 0.4304993152618408\n",
      "Alpha: 0.4304993152618408\n",
      "Alpha: 0.43001776933670044\n",
      "Alpha: 0.43001776933670044\n",
      "Alpha: 0.4295382797718048\n",
      "Alpha: 0.4295382797718048\n",
      "Alpha: 0.4290618896484375\n",
      "Alpha: 0.4290618896484375\n",
      "Alpha: 0.4285881519317627\n",
      "Alpha: 0.4285881519317627\n",
      "Alpha: 0.4281197786331177\n",
      "Alpha: 0.4281197786331177\n",
      "Alpha: 0.4276517927646637\n",
      "Alpha: 0.4276517927646637\n",
      "Alpha: 0.42718154191970825\n",
      "Alpha: 0.42718154191970825\n",
      "Alpha: 0.4267105758190155\n",
      "Alpha: 0.4267105758190155\n",
      "Alpha: 0.42624059319496155\n",
      "Alpha: 0.42624059319496155\n",
      "Alpha: 0.425769180059433\n",
      "Alpha: 0.425769180059433\n",
      "Alpha: 0.4252999424934387\n",
      "Alpha: 0.4252999424934387\n",
      "Alpha: 0.4248298108577728\n",
      "Alpha: 0.4248298108577728\n",
      "Alpha: 0.424360454082489\n",
      "Alpha: 0.424360454082489\n",
      "Alpha: 0.42389318346977234\n",
      "Alpha: 0.42389318346977234\n",
      "Alpha: 0.42342865467071533\n",
      "Alpha: 0.42342865467071533\n",
      "Alpha: 0.4229666590690613\n",
      "Alpha: 0.4229666590690613\n",
      "Alpha: 0.42250964045524597\n",
      "Alpha: 0.42250964045524597\n",
      "Alpha: 0.42205387353897095\n",
      "Alpha: 0.42205387353897095\n",
      "Alpha: 0.42160457372665405\n",
      "Alpha: 0.42160457372665405\n",
      "Alpha: 0.4211544096469879\n",
      "Alpha: 0.4211544096469879\n",
      "Alpha: 0.4207034707069397\n",
      "Alpha: 0.4207034707069397\n",
      "Alpha: 0.420253187417984\n",
      "Alpha: 0.420253187417984\n",
      "Alpha: 0.4198002219200134\n",
      "Alpha: 0.4198002219200134\n",
      "Alpha: 0.4193477928638458\n",
      "Alpha: 0.4193477928638458\n",
      "Alpha: 0.4188947081565857\n",
      "Alpha: 0.4188947081565857\n",
      "Alpha: 0.41844213008880615\n",
      "Alpha: 0.41844213008880615\n",
      "Alpha: 0.4179869592189789\n",
      "Alpha: 0.4179869592189789\n",
      "Alpha: 0.41753333806991577\n",
      "Alpha: 0.41753333806991577\n",
      "Alpha: 0.41708165407180786\n",
      "Alpha: 0.41708165407180786\n",
      "Alpha: 0.4166344106197357\n",
      "Alpha: 0.4166344106197357\n",
      "Alpha: 0.4161897301673889\n",
      "Alpha: 0.4161897301673889\n",
      "Alpha: 0.4157455265522003\n",
      "Alpha: 0.4157455265522003\n",
      "Alpha: 0.4153025150299072\n",
      "Alpha: 0.4153025150299072\n",
      "Alpha: 0.41486212611198425\n",
      "Alpha: 0.41486212611198425\n",
      "Alpha: 0.4144216775894165\n",
      "Alpha: 0.4144216775894165\n",
      "Alpha: 0.41397881507873535\n",
      "Alpha: 0.41397881507873535\n",
      "Alpha: 0.4135352075099945\n",
      "Alpha: 0.4135352075099945\n",
      "Alpha: 0.4130936861038208\n",
      "Alpha: 0.4130936861038208\n",
      "Alpha: 0.4126470386981964\n",
      "Alpha: 0.4126470386981964\n",
      "Alpha: 0.4121938943862915\n",
      "Alpha: 0.4121938943862915\n",
      "Alpha: 0.41173750162124634\n",
      "Alpha: 0.41173750162124634\n",
      "Alpha: 0.4112776815891266\n",
      "Alpha: 0.4112776815891266\n",
      "Alpha: 0.41081503033638\n",
      "Alpha: 0.41081503033638\n",
      "Alpha: 0.4103551506996155\n",
      "Alpha: 0.4103551506996155\n",
      "Alpha: 0.40989789366722107\n",
      "Alpha: 0.40989789366722107\n",
      "Alpha: 0.40944692492485046\n",
      "Alpha: 0.40944692492485046\n",
      "Alpha: 0.4090001583099365\n",
      "Alpha: 0.4090001583099365\n",
      "Alpha: 0.4085563123226166\n",
      "Alpha: 0.4085563123226166\n",
      "Alpha: 0.40811529755592346\n",
      "Alpha: 0.40811529755592346\n",
      "Alpha: 0.40767702460289\n",
      "Alpha: 0.40767702460289\n",
      "Alpha: 0.4072403013706207\n",
      "Alpha: 0.4072403013706207\n",
      "Alpha: 0.40680256485939026\n",
      "Alpha: 0.40680256485939026\n",
      "Alpha: 0.40636399388313293\n",
      "Alpha: 0.40636399388313293\n",
      "Alpha: 0.4059271514415741\n",
      "Alpha: 0.4059271514415741\n",
      "Alpha: 0.40549349784851074\n",
      "Alpha: 0.40549349784851074\n",
      "Alpha: 0.40506118535995483\n",
      "Alpha: 0.40506118535995483\n",
      "Alpha: 0.4046303629875183\n",
      "Alpha: 0.4046303629875183\n",
      "Alpha: 0.4042041301727295\n",
      "Alpha: 0.4042041301727295\n",
      "Alpha: 0.4037809371948242\n",
      "Alpha: 0.4037809371948242\n",
      "Score:   -78.06\n",
      "Alpha: 0.4033614695072174\n",
      "Alpha: 0.4033614695072174\n",
      "Alpha: 0.4029465317726135\n",
      "Alpha: 0.4029465317726135\n",
      "Alpha: 0.4025366008281708\n",
      "Alpha: 0.4025366008281708\n",
      "Alpha: 0.4021245837211609\n",
      "Alpha: 0.4021245837211609\n",
      "Alpha: 0.4017081558704376\n",
      "Alpha: 0.4017081558704376\n",
      "Alpha: 0.4012880027294159\n",
      "Alpha: 0.4012880027294159\n",
      "Alpha: 0.40086477994918823\n",
      "Alpha: 0.40086477994918823\n",
      "Alpha: 0.40043795108795166\n",
      "Alpha: 0.40043795108795166\n",
      "Alpha: 0.4000122547149658\n",
      "Alpha: 0.4000122547149658\n",
      "Alpha: 0.3995859622955322\n",
      "Alpha: 0.3995859622955322\n",
      "Alpha: 0.3991650640964508\n",
      "Alpha: 0.3991650640964508\n",
      "Alpha: 0.39875081181526184\n",
      "Alpha: 0.39875081181526184\n",
      "Alpha: 0.39834168553352356\n",
      "Alpha: 0.39834168553352356\n",
      "Alpha: 0.39793267846107483\n",
      "Alpha: 0.39793267846107483\n",
      "Alpha: 0.39752835035324097\n",
      "Alpha: 0.39752835035324097\n",
      "Alpha: 0.39712560176849365\n",
      "Alpha: 0.39712560176849365\n",
      "Alpha: 0.3967202305793762\n",
      "Alpha: 0.3967202305793762\n",
      "Alpha: 0.39631566405296326\n",
      "Alpha: 0.39631566405296326\n",
      "Alpha: 0.39591339230537415\n",
      "Alpha: 0.39591339230537415\n",
      "Alpha: 0.3955124020576477\n",
      "Alpha: 0.3955124020576477\n",
      "Alpha: 0.3951122462749481\n",
      "Alpha: 0.3951122462749481\n",
      "Alpha: 0.39471471309661865\n",
      "Alpha: 0.39471471309661865\n",
      "Alpha: 0.39431625604629517\n",
      "Alpha: 0.39431625604629517\n",
      "Alpha: 0.39392298460006714\n",
      "Alpha: 0.39392298460006714\n",
      "Alpha: 0.3935311734676361\n",
      "Alpha: 0.3935311734676361\n",
      "Alpha: 0.39313942193984985\n",
      "Alpha: 0.39313942193984985\n",
      "Alpha: 0.39275062084198\n",
      "Alpha: 0.39275062084198\n",
      "Alpha: 0.3923616409301758\n",
      "Alpha: 0.3923616409301758\n",
      "Alpha: 0.39196905493736267\n",
      "Alpha: 0.39196905493736267\n",
      "Alpha: 0.39157286286354065\n",
      "Alpha: 0.39157286286354065\n",
      "Alpha: 0.39117497205734253\n",
      "Alpha: 0.39117497205734253\n",
      "Alpha: 0.3907760977745056\n",
      "Alpha: 0.3907760977745056\n",
      "Alpha: 0.3903743326663971\n",
      "Alpha: 0.3903743326663971\n",
      "Alpha: 0.38997089862823486\n",
      "Alpha: 0.38997089862823486\n",
      "Alpha: 0.3895677626132965\n",
      "Alpha: 0.3895677626132965\n",
      "Alpha: 0.38916143774986267\n",
      "Alpha: 0.38916143774986267\n",
      "Alpha: 0.3887563645839691\n",
      "Alpha: 0.3887563645839691\n",
      "Alpha: 0.3883514702320099\n",
      "Alpha: 0.3883514702320099\n",
      "Alpha: 0.3879469633102417\n",
      "Alpha: 0.3879469633102417\n",
      "Alpha: 0.38754281401634216\n",
      "Alpha: 0.38754281401634216\n",
      "Alpha: 0.38713937997817993\n",
      "Alpha: 0.38713937997817993\n",
      "Alpha: 0.38673797249794006\n",
      "Alpha: 0.38673797249794006\n",
      "Alpha: 0.3863406479358673\n",
      "Alpha: 0.3863406479358673\n",
      "Alpha: 0.3859490156173706\n",
      "Alpha: 0.3859490156173706\n",
      "Alpha: 0.38555994629859924\n",
      "Alpha: 0.38555994629859924\n",
      "Alpha: 0.38517430424690247\n",
      "Alpha: 0.38517430424690247\n",
      "Alpha: 0.3847871422767639\n",
      "Alpha: 0.3847871422767639\n",
      "Alpha: 0.38440102338790894\n",
      "Alpha: 0.38440102338790894\n",
      "Alpha: 0.384013295173645\n",
      "Alpha: 0.384013295173645\n",
      "Alpha: 0.38362663984298706\n",
      "Alpha: 0.38362663984298706\n",
      "Alpha: 0.3832375109195709\n",
      "Alpha: 0.3832375109195709\n",
      "Alpha: 0.3828478157520294\n",
      "Alpha: 0.3828478157520294\n",
      "Alpha: 0.38245445489883423\n",
      "Alpha: 0.38245445489883423\n",
      "Alpha: 0.38205862045288086\n",
      "Alpha: 0.38205862045288086\n",
      "Alpha: 0.38165953755378723\n",
      "Alpha: 0.38165953755378723\n",
      "Alpha: 0.3812578618526459\n",
      "Alpha: 0.3812578618526459\n",
      "Alpha: 0.38085800409317017\n",
      "Alpha: 0.38085800409317017\n",
      "Alpha: 0.380460649728775\n",
      "Alpha: 0.380460649728775\n",
      "Alpha: 0.3800629675388336\n",
      "Alpha: 0.3800629675388336\n",
      "Alpha: 0.3796614706516266\n",
      "Alpha: 0.3796614706516266\n",
      "Alpha: 0.3792608678340912\n",
      "Alpha: 0.3792608678340912\n",
      "Alpha: 0.3788647949695587\n",
      "Alpha: 0.3788647949695587\n",
      "Alpha: 0.3784692585468292\n",
      "Alpha: 0.3784692585468292\n",
      "Alpha: 0.37807518243789673\n",
      "Alpha: 0.37807518243789673\n",
      "Alpha: 0.37768271565437317\n",
      "Alpha: 0.37768271565437317\n",
      "Alpha: 0.37729278206825256\n",
      "Alpha: 0.37729278206825256\n",
      "Alpha: 0.37690454721450806\n",
      "Alpha: 0.37690454721450806\n",
      "Alpha: 0.3765178918838501\n",
      "Alpha: 0.3765178918838501\n",
      "Alpha: 0.37613287568092346\n",
      "Alpha: 0.37613287568092346\n",
      "Alpha: 0.37574833631515503\n",
      "Alpha: 0.37574833631515503\n",
      "Alpha: 0.3753669261932373\n",
      "Alpha: 0.3753669261932373\n",
      "Alpha: 0.37498748302459717\n",
      "Alpha: 0.37498748302459717\n",
      "Alpha: 0.3746132552623749\n",
      "Alpha: 0.3746132552623749\n",
      "Alpha: 0.37423861026763916\n",
      "Alpha: 0.37423861026763916\n",
      "Alpha: 0.37386417388916016\n",
      "Alpha: 0.37386417388916016\n",
      "Alpha: 0.37348508834838867\n",
      "Alpha: 0.37348508834838867\n",
      "Alpha: 0.37310561537742615\n",
      "Alpha: 0.37310561537742615\n",
      "Alpha: 0.37272265553474426\n",
      "Alpha: 0.37272265553474426\n",
      "Alpha: 0.37233853340148926\n",
      "Alpha: 0.37233853340148926\n",
      "Alpha: 0.3719569742679596\n",
      "Alpha: 0.3719569742679596\n",
      "Score:  -287.66\n",
      "Alpha: 0.3715757429599762\n",
      "Alpha: 0.3715757429599762\n",
      "Alpha: 0.3711965084075928\n",
      "Alpha: 0.3711965084075928\n",
      "Alpha: 0.37081626057624817\n",
      "Alpha: 0.37081626057624817\n",
      "Alpha: 0.37043496966362\n",
      "Alpha: 0.37043496966362\n",
      "Alpha: 0.37005528807640076\n",
      "Alpha: 0.37005528807640076\n",
      "Alpha: 0.36967557668685913\n",
      "Alpha: 0.36967557668685913\n",
      "Alpha: 0.36929595470428467\n",
      "Alpha: 0.36929595470428467\n",
      "Alpha: 0.3689170479774475\n",
      "Alpha: 0.3689170479774475\n",
      "Alpha: 0.36854302883148193\n",
      "Alpha: 0.36854302883148193\n",
      "Alpha: 0.3681734502315521\n",
      "Alpha: 0.3681734502315521\n",
      "Alpha: 0.3678077757358551\n",
      "Alpha: 0.3678077757358551\n",
      "Alpha: 0.36744099855422974\n",
      "Alpha: 0.36744099855422974\n",
      "Alpha: 0.3670729994773865\n",
      "Alpha: 0.3670729994773865\n",
      "Alpha: 0.36670830845832825\n",
      "Alpha: 0.36670830845832825\n",
      "Alpha: 0.3663392961025238\n",
      "Alpha: 0.3663392961025238\n",
      "Alpha: 0.3659670650959015\n",
      "Alpha: 0.3659670650959015\n",
      "Alpha: 0.36559340357780457\n",
      "Alpha: 0.36559340357780457\n",
      "Alpha: 0.3652167320251465\n",
      "Alpha: 0.3652167320251465\n",
      "Alpha: 0.36483970284461975\n",
      "Alpha: 0.36483970284461975\n",
      "Alpha: 0.3644605576992035\n",
      "Alpha: 0.3644605576992035\n",
      "Alpha: 0.36407965421676636\n",
      "Alpha: 0.36407965421676636\n",
      "Alpha: 0.36370131373405457\n",
      "Alpha: 0.36370131373405457\n",
      "Alpha: 0.3633231222629547\n",
      "Alpha: 0.3633231222629547\n",
      "Alpha: 0.36294877529144287\n",
      "Alpha: 0.36294877529144287\n",
      "Alpha: 0.362578809261322\n",
      "Alpha: 0.362578809261322\n",
      "Alpha: 0.36221182346343994\n",
      "Alpha: 0.36221182346343994\n",
      "Alpha: 0.3618501126766205\n",
      "Alpha: 0.3618501126766205\n",
      "Alpha: 0.3614879548549652\n",
      "Alpha: 0.3614879548549652\n",
      "Alpha: 0.36112433671951294\n",
      "Alpha: 0.36112433671951294\n",
      "Alpha: 0.360761821269989\n",
      "Alpha: 0.360761821269989\n",
      "Alpha: 0.3603994846343994\n",
      "Alpha: 0.3603994846343994\n",
      "Alpha: 0.3600391447544098\n",
      "Alpha: 0.3600391447544098\n",
      "Alpha: 0.3596796989440918\n",
      "Alpha: 0.3596796989440918\n",
      "Alpha: 0.3593193292617798\n",
      "Alpha: 0.3593193292617798\n",
      "Alpha: 0.35895857214927673\n",
      "Alpha: 0.35895857214927673\n",
      "Alpha: 0.35859712958335876\n",
      "Alpha: 0.35859712958335876\n",
      "Alpha: 0.35823482275009155\n",
      "Alpha: 0.35823482275009155\n",
      "Alpha: 0.3578692078590393\n",
      "Alpha: 0.3578692078590393\n",
      "Alpha: 0.35750308632850647\n",
      "Alpha: 0.35750308632850647\n",
      "Alpha: 0.3571373224258423\n",
      "Alpha: 0.3571373224258423\n",
      "Alpha: 0.3567723035812378\n",
      "Alpha: 0.3567723035812378\n",
      "Alpha: 0.35640668869018555\n",
      "Alpha: 0.35640668869018555\n",
      "Alpha: 0.3560420274734497\n",
      "Alpha: 0.3560420274734497\n",
      "Alpha: 0.3556802272796631\n",
      "Alpha: 0.3556802272796631\n",
      "Alpha: 0.35532066226005554\n",
      "Alpha: 0.35532066226005554\n",
      "Alpha: 0.35496076941490173\n",
      "Alpha: 0.35496076941490173\n",
      "Alpha: 0.3546023368835449\n",
      "Alpha: 0.3546023368835449\n",
      "Alpha: 0.3542453646659851\n",
      "Alpha: 0.3542453646659851\n",
      "Alpha: 0.35389068722724915\n",
      "Alpha: 0.35389068722724915\n",
      "Alpha: 0.35353901982307434\n",
      "Alpha: 0.35353901982307434\n",
      "Alpha: 0.3531874418258667\n",
      "Alpha: 0.3531874418258667\n",
      "Alpha: 0.35283640027046204\n",
      "Alpha: 0.35283640027046204\n",
      "Alpha: 0.3524838387966156\n",
      "Alpha: 0.3524838387966156\n",
      "Alpha: 0.35212650895118713\n",
      "Alpha: 0.35212650895118713\n",
      "Alpha: 0.3517647981643677\n",
      "Alpha: 0.3517647981643677\n",
      "Alpha: 0.35139966011047363\n",
      "Alpha: 0.35139966011047363\n",
      "Alpha: 0.3510327637195587\n",
      "Alpha: 0.3510327637195587\n",
      "Alpha: 0.35066214203834534\n",
      "Alpha: 0.35066214203834534\n",
      "Alpha: 0.3502900004386902\n",
      "Alpha: 0.3502900004386902\n",
      "Alpha: 0.34991979598999023\n",
      "Alpha: 0.34991979598999023\n",
      "Alpha: 0.34955137968063354\n",
      "Alpha: 0.34955137968063354\n",
      "Alpha: 0.3491857647895813\n",
      "Alpha: 0.3491857647895813\n",
      "Alpha: 0.34882280230522156\n",
      "Alpha: 0.34882280230522156\n",
      "Alpha: 0.3484635055065155\n",
      "Alpha: 0.3484635055065155\n",
      "Alpha: 0.3481059968471527\n",
      "Alpha: 0.3481059968471527\n",
      "Alpha: 0.34775054454803467\n",
      "Alpha: 0.34775054454803467\n",
      "Alpha: 0.34739935398101807\n",
      "Alpha: 0.34739935398101807\n",
      "Alpha: 0.3470514416694641\n",
      "Alpha: 0.3470514416694641\n",
      "Alpha: 0.3467070758342743\n",
      "Alpha: 0.3467070758342743\n",
      "Alpha: 0.3463664650917053\n",
      "Alpha: 0.3463664650917053\n",
      "Alpha: 0.3460264503955841\n",
      "Alpha: 0.3460264503955841\n",
      "Alpha: 0.3456893265247345\n",
      "Alpha: 0.3456893265247345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m alphaLoss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mlogAlpha\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m*\u001b[39m(logProbabilities \u001b[38;5;241m+\u001b[39m target_entropy))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     85\u001b[0m alphaOptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 86\u001b[0m \u001b[43malphaLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m alphaOptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     88\u001b[0m alpha \u001b[38;5;241m=\u001b[39m logAlpha\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "actor = Actor(envs).to(device)\n",
    "QNet1 = SoftQNetwork(envs).to(device)\n",
    "QNet2 = SoftQNetwork(envs).to(device)\n",
    "QNet1_target = SoftQNetwork(envs).to(device)\n",
    "QNet2_target = SoftQNetwork(envs).to(device)\n",
    "QNet1_target.load_state_dict(QNet1.state_dict())\n",
    "QNet2_target.load_state_dict(QNet2.state_dict())\n",
    "QNetsOptimizer = optim.Adam(list(QNet1.parameters()) + list(QNet2.parameters()), lr=q_lr)\n",
    "actorOptimizer = optim.Adam(list(actor.parameters()), lr=policy_lr)\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if autoEntropy:\n",
    "    target_entropy = -torch.tensor(envs.single_action_space.shape).prod().item()\n",
    "    logAlpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = logAlpha.exp().item()\n",
    "    alphaOptimizer = optim.Adam([logAlpha], lr=q_lr)\n",
    "\n",
    "experiences = ReplayBuffer(\n",
    "    buffer_size,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "\n",
    "allScores = []\n",
    "obs, _ = envs.reset(seed=seed)\n",
    "for globalStep in range(totalTimesteps):\n",
    "    actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "    actions = actions.detach().cpu().numpy()\n",
    "\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    experiences.add(obs, next_obs, actions, rewards, terminations, infos)\n",
    "    obs = next_obs\n",
    "\n",
    "    done = np.logical_or.reduce([terminations, truncations])\n",
    "    totalEpisodicRewards += torch.tensor(rewards).to(device)\n",
    "    for finalScore in totalEpisodicRewards[done]:\n",
    "        print(f\"Score: {finalScore:>8.2f}\")\n",
    "        allScores.append(finalScore.item())\n",
    "    totalEpisodicRewards[done] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sampledExperiences = experiences.sample(min(batch_size, experiences.size()))\n",
    "    with torch.no_grad():\n",
    "        nextStateActions, nextStateLogProbs, _ = actor.get_action(sampledExperiences.next_observations)\n",
    "        QFunction1NextTarget = QNet1_target(sampledExperiences.next_observations, nextStateActions)\n",
    "        QFunction2NextTarget = QNet2_target(sampledExperiences.next_observations, nextStateActions)\n",
    "        minQNextTarget = torch.min(QFunction1NextTarget, QFunction2NextTarget) - alpha * nextStateLogProbs\n",
    "        nextQValue = sampledExperiences.rewards.flatten() + (1 - sampledExperiences.dones.flatten()) * gamma * (minQNextTarget).view(-1)\n",
    "\n",
    "\n",
    "    QFunction1ActionValues = QNet1(sampledExperiences.observations, sampledExperiences.actions).view(-1)\n",
    "    QFunction2ActionValues = QNet2(sampledExperiences.observations, sampledExperiences.actions).view(-1)\n",
    "    QFunction1Loss = F.mse_loss(QFunction1ActionValues, nextQValue)\n",
    "    QFunction2Loss = F.mse_loss(QFunction2ActionValues, nextQValue)\n",
    "    QFunctionsTotalLoss = QFunction1Loss + QFunction2Loss\n",
    "\n",
    "    QNetsOptimizer.zero_grad()\n",
    "    QFunctionsTotalLoss.backward()\n",
    "    QNetsOptimizer.step()\n",
    "\n",
    "    if globalStep % policyFrequency == 0:\n",
    "        for i in range(policyFrequency):\n",
    "            if i > 0:\n",
    "                # Sample new experiences if we make multiple updates\n",
    "                sampledExperiences = experiences.sample(min(batch_size, experiences.size()))\n",
    "            actions, logProbabilities, _ = actor.get_action(sampledExperiences.observations)\n",
    "            QFunction1Evaluation = QNet1(sampledExperiences.observations, actions)\n",
    "            QFunction2Evaluation = QNet2(sampledExperiences.observations, actions)\n",
    "            minQEvalutaion = torch.min(QFunction1Evaluation, QFunction2Evaluation)\n",
    "            actorLoss = ((alpha * logProbabilities) - minQEvalutaion).mean()\n",
    "\n",
    "            actorOptimizer.zero_grad()\n",
    "            actorLoss.backward()\n",
    "            actorOptimizer.step()\n",
    "\n",
    "            if autoEntropy:\n",
    "                with torch.no_grad():\n",
    "                    _, logProbabilities, _ = actor.get_action(sampledExperiences.observations)\n",
    "                alphaLoss = (-logAlpha.exp()*(logProbabilities + target_entropy)).mean()\n",
    "\n",
    "                alphaOptimizer.zero_grad()\n",
    "                alphaLoss.backward()\n",
    "                alphaOptimizer.step()\n",
    "                alpha = logAlpha.exp().item()\n",
    "    print(f\"Alpha: {alpha}\")\n",
    "\n",
    "    if globalStep % QNetworkFrequency == 0:\n",
    "        for param, targetParam in zip(QNet1.parameters(), QNet1_target.parameters()):\n",
    "            targetParam.data.copy_(tau*param.data + (1 - tau)*targetParam.data)\n",
    "        for param, targetParam in zip(QNet2.parameters(), QNet2_target.parameters()):\n",
    "            targetParam.data.copy_(tau*param.data + (1 - tau)*targetParam.data)\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [2] doesn't match the broadcast shape [1, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m totalReward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     actions, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m     next_obs, rewards, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mActor.get_action\u001b[1;34m(self, x, evaluation)\u001b[0m\n\u001b[0;32m     56\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m policyDistribution\u001b[38;5;241m.\u001b[39mlog_prob(actionSample)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# print(f\"log_prob.shape {log_prob.shape}, log_prob {log_prob}\")\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# print(f\"tanhAction.shape {tanhAction.shape}, tanhAction {tanhAction}\")\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# print(f\"action_scale.shape {self.action_scale.shape}, action_scale {self.action_scale}\")\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print(f\"Trying to add to log prob log of self.action_scale * (1 - tanhAction.pow(2)) + 1e-6: {self.action_scale * (1 - tanhAction.pow(2)) + 1e-6}\")\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Enforcing Action Bound\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m log_prob \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_scale \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m tanhAction\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(f\"Added it, so now log_prob is {log_prob} of shape {log_prob.shape}\")\u001b[39;00m\n\u001b[0;32m     64\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [2] doesn't match the broadcast shape [1, 2]"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "environment = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "obs, _ = environment.reset()\n",
    "totalReward = 0\n",
    "\n",
    "while True:\n",
    "    actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "    actions = actions.detach().cpu().numpy()\n",
    "    next_obs, rewards, terminated, truncated, _ = environment.step(actions)\n",
    "    obs = next_obs\n",
    "    image = environment.render()\n",
    "    images.append(image)\n",
    "    totalReward += torch.tensor(rewards).to(device)\n",
    "    if terminated or truncated:\n",
    "        print(f\"Score: {finalScore:>8.2f}\")\n",
    "        break\n",
    "# saveVideo(images, f\"Testing-({totalReward:.0f}).mp4\", 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
